{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# [UW-Madison GI Tract Image Segmentation](https://www.kaggle.com/competitions/uw-madison-gi-tract-image-segmentation/)\n> Track healthy organs in medical scans to improve cancer treatment\n\n<img src=\"https://storage.googleapis.com/kaggle-competitions/kaggle/27923/logos/header.png?t=2021-06-02-20-30-25\">","metadata":{}},{"cell_type":"markdown","source":"# ⚽ Methodlogy\n<img src=\"https://i.ibb.co/sgsPf4v/Capture.png\" width=800>\n<img src=\"https://i.ibb.co/KKtZ7Gn/Picture1-3d.png\" width=500>\n\n* In this notebook I'll demonstrate how to train using 2.5D images with **Unet** model using PyTorch.\n* 2.5D images take leverage of the extra depth information like our typical RGB image.\n* In this notebook I'll be using 3 channels with 2 strides for 2.5D images\n* Instead of Resize I'll be using Padding to avoid info loss.\n* For mask I'll be using pre-computed 2.5D images & mask from [here](https://www.kaggle.com/code/awsaf49/uwmgi-2-5d-stride-2-dataset)\n* As there are overlaps between **Stomach**, **Large Bowel** & **Small Bowel** classes, this is a **MultiLabel Segmentation** task, so final activaion should be `sigmoid` instead of `softmax`.\n* For data split I'll be using **StratifiedGroupFold** to avoid data leakage due to `case` and to stratify `empty` and `non-empty` mask cases.\n* You can play with different models and losses.","metadata":{}},{"cell_type":"markdown","source":"# 🚩 Version Info","metadata":{}},{"cell_type":"markdown","source":"# 📒 Notebooks\n📌 **2.5D-TransUnet**:\n* Train: [UWMGI: TransUNet 2.5D [Train] [TF]](https://www.kaggle.com/code/awsaf49/uwmgi-transunet-2-5d-train-tf)\n\n📌 **2.5D**:\n* Train: [UWMGI: 2.5D [Train] [PyTorch]](https://www.kaggle.com/awsaf49/uwmgi-2-5d-train-pytorch/)\n* Infer: [UWMGI: 2.5D [Infer] [PyTorch]](https://www.kaggle.com/awsaf49/uwmgi-2-5d-infer-pytorch/)\n* Data: [UWMGI: 2.5D stride=2 Data](https://www.kaggle.com/code/awsaf49/uwmgi-2-5d-stride-2-data/)\n\n📌 **UNet**:\n* Train: [UWMGI: Unet [Train] [PyTorch]](https://www.kaggle.com/code/awsaf49/uwmgi-unet-train-pytorch/)\n* Infer: [UWMGI: Unet [Infer] [PyTorch]](https://www.kaggle.com/code/awsaf49/uwmgi-unet-infer-pytorch/)\n\n📌 **MMDetection**:\n* Train: [UWMGI: MMDetection [Train]](https://www.kaggle.com/code/awsaf49/uwmgi-mmdetection-train)\n\n📌 **Data/Dataset**:\n* Data: [UWMGI: Mask Data](https://www.kaggle.com/datasets/awsaf49/uwmgi-mask-data)\n* Dataset: [UWMGI: Mask Dataset](https://www.kaggle.com/datasets/awsaf49/uwmgi-mask-dataset)","metadata":{}},{"cell_type":"markdown","source":"## Please Upvote if you Find this Useful :)","metadata":{}},{"cell_type":"markdown","source":"# 🛠 Install Libraries","metadata":{}},{"cell_type":"code","source":"!pip install -q ../input/pytorch-segmentation-models-lib/pretrainedmodels-0.7.4/pretrainedmodels-0.7.4\n!pip install -q ../input/pytorch-segmentation-models-lib/efficientnet_pytorch-0.6.3/efficientnet_pytorch-0.6.3\n!pip install -q ../input/pytorch-segmentation-models-lib/timm-0.4.12-py3-none-any.whl\n!pip install -q ../input/pytorch-segmentation-models-lib/segmentation_models_pytorch-0.2.0-py3-none-any.whl","metadata":{"_kg_hide-output":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-07-03T02:00:35.264987Z","iopub.execute_input":"2022-07-03T02:00:35.265392Z","iopub.status.idle":"2022-07-03T02:02:29.831125Z","shell.execute_reply.started":"2022-07-03T02:00:35.265356Z","shell.execute_reply":"2022-07-03T02:02:29.830264Z"},"trusted":true},"execution_count":167,"outputs":[{"name":"stdout","text":"\u001b[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n\u001b[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# 📚 Import Libraries ","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\npd.options.plotting.backend = \"plotly\"\nimport random\nfrom glob import glob\nimport os, shutil\nfrom tqdm import tqdm\ntqdm.pandas()\nimport time\nimport copy\nimport joblib\nfrom collections import defaultdict\nimport gc\nfrom IPython import display as ipd\n\n# visualization\nimport cv2\nimport matplotlib.pyplot as plt\n\n# Sklearn\nfrom sklearn.model_selection import StratifiedKFold, KFold\n\n# PyTorch \nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.cuda import amp\nimport torch.nn.functional as F\n\nimport timm\n\n# Albumentations for augmentations\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\n# For colored terminal text\nfrom colorama import Fore, Back, Style\nc_  = Fore.GREEN\nsr_ = Style.RESET_ALL\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# For descriptive error messages\nos.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-07-03T02:02:29.833546Z","iopub.execute_input":"2022-07-03T02:02:29.833843Z","iopub.status.idle":"2022-07-03T02:02:29.844315Z","shell.execute_reply.started":"2022-07-03T02:02:29.833807Z","shell.execute_reply":"2022-07-03T02:02:29.843498Z"},"trusted":true},"execution_count":168,"outputs":[]},{"cell_type":"markdown","source":"# ⚙️ Configuration ","metadata":{}},{"cell_type":"code","source":"class CFG:\n    seed          = 101\n    debug         = False # set debug=False for Full Training\n    exp_name      = 'v4'\n    comment       = 'unet-efficientnet_b0-320x384'\n    model_name    = 'Unet'\n    backbone      = 'timm-mobilenetv3_large_100'\n    train_bs      = 8\n    valid_bs      = train_bs*2\n    img_size      = [320, 384]\n    epochs        = 16\n    lr            = 2e-3\n    scheduler     = 'CosineAnnealingLR'\n    min_lr        = 1e-6\n    T_max         = int(30000/train_bs*epochs)+50\n    T_0           = 25\n    warmup_epochs = 0\n    wd            = 1e-6\n    n_accumulate  = max(1, 64//train_bs)\n    n_fold        = 2\n    folds         = [0]\n    num_classes   = 3\n    thr           = 0.550\n    device        = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-07-03T02:26:20.181943Z","iopub.execute_input":"2022-07-03T02:26:20.182195Z","iopub.status.idle":"2022-07-03T02:26:20.188951Z","shell.execute_reply.started":"2022-07-03T02:26:20.182167Z","shell.execute_reply":"2022-07-03T02:26:20.188242Z"},"trusted":true},"execution_count":191,"outputs":[]},{"cell_type":"markdown","source":"# ❗ Reproducibility","metadata":{}},{"cell_type":"code","source":"def set_seed(seed = 42):\n    '''Sets the seed of the entire notebook so results are the same every time we run.\n    This is for REPRODUCIBILITY.'''\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    # When running on the CuDNN backend, two further options must be set\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    # Set a fixed value for the hash seed\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    print('> SEEDING DONE')\n    \nset_seed(CFG.seed)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-07-03T02:02:29.865317Z","iopub.execute_input":"2022-07-03T02:02:29.865903Z","iopub.status.idle":"2022-07-03T02:02:29.876176Z","shell.execute_reply.started":"2022-07-03T02:02:29.865864Z","shell.execute_reply":"2022-07-03T02:02:29.875404Z"},"trusted":true},"execution_count":170,"outputs":[{"name":"stdout","text":"> SEEDING DONE\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# 🔨 Utility","metadata":{}},{"cell_type":"code","source":"def get_metadata(row):\n    data = row['id'].split('_')\n    case = int(data[0].replace('case',''))\n    day = int(data[1].replace('day',''))\n    slice_ = int(data[-1])\n    row['case'] = case\n    row['day'] = day\n    row['slice'] = slice_\n    return row\n\ndef path2info(row):\n    path = row['image_path']\n    data = path.split('/')\n    slice_ = int(data[-1].split('_')[1])\n    case = int(data[-3].split('_')[0].replace('case',''))\n    day = int(data[-3].split('_')[1].replace('day',''))\n    width = int(data[-1].split('_')[2])\n    height = int(data[-1].split('_')[3])\n    row['height'] = height\n    row['width'] = width\n    row['case'] = case\n    row['day'] = day\n    row['slice'] = slice_\n#     row['id'] = f'case{case}_day{day}_slice_{slice_}'\n    return row","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-07-03T02:02:29.877640Z","iopub.execute_input":"2022-07-03T02:02:29.878464Z","iopub.status.idle":"2022-07-03T02:02:29.889980Z","shell.execute_reply.started":"2022-07-03T02:02:29.878427Z","shell.execute_reply":"2022-07-03T02:02:29.889193Z"},"trusted":true},"execution_count":171,"outputs":[]},{"cell_type":"code","source":"def load_img(path, size=CFG.img_size):\n    img = cv2.imread(path, cv2.IMREAD_UNCHANGED)\n    shape0 = np.array(img.shape[:2])\n    resize = np.array(size)\n    if np.any(shape0!=resize):\n        diff = resize - shape0\n        pad0 = diff[0]\n        pad1 = diff[1]\n        pady = [pad0//2, pad0//2 + pad0%2]\n        padx = [pad1//2, pad1//2 + pad1%2]\n        img = np.pad(img, [pady, padx])\n        img = img.reshape((*resize))\n    return img, shape0\n\ndef load_imgs(img_paths, size=CFG.img_size):\n    imgs = np.zeros((*size, len(img_paths)), dtype=np.float32)\n    for i, img_path in enumerate(img_paths):\n        if i==0:\n            img, shape0 = load_img(img_path, size=size)\n        else:\n            img, _ = load_img(img_path, size=size)\n        img = img.astype('float32') # original is uint16\n        mx = np.max(img)\n        if mx:\n            img/=mx # scale image to [0, 1]\n        imgs[..., i]+=img\n    return imgs, shape0\n\ndef load_msk(path, size=CFG.img_size):\n    msk = np.load(path)\n    shape0 = np.array(msk.shape[:2])\n    resize = np.array(size)\n    if np.any(shape0!=resize):\n        diff = resize - shape0\n        pad0 = diff[0]\n        pad1 = diff[1]\n        pady = [pad0//2, pad0//2 + pad0%2]\n        padx = [pad1//2, pad1//2 + pad1%2]\n        msk = np.pad(msk, [pady, padx, [0,0]])\n        msk = msk.reshape((*resize, 3))\n    msk = msk.astype('float32')\n    msk/=255.0\n    return msk\n\ndef show_img(img, mask=None):\n    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n    img = clahe.apply(img)\n    plt.imshow(img, cmap='bone')\n    \n    if mask is not None:\n        # plt.imshow(np.ma.masked_where(mask!=1, mask), alpha=0.5, cmap='autumn')\n        plt.imshow(mask, alpha=0.5)\n        handles = [Rectangle((0,0),1,1, color=_c) for _c in [(0.667,0.0,0.0), (0.0,0.667,0.0), (0.0,0.0,0.667)]]\n        labels = [\"Large Bowel\", \"Small Bowel\", \"Stomach\"]\n        plt.legend(handles,labels)\n    plt.axis('off')","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-07-03T02:02:29.891029Z","iopub.execute_input":"2022-07-03T02:02:29.893321Z","iopub.status.idle":"2022-07-03T02:02:29.910587Z","shell.execute_reply.started":"2022-07-03T02:02:29.893282Z","shell.execute_reply":"2022-07-03T02:02:29.909775Z"},"trusted":true},"execution_count":172,"outputs":[]},{"cell_type":"code","source":"# ref: https://www.kaggle.com/paulorzp/run-length-encode-and-decode\ndef rle_decode(mask_rle, shape):\n    '''\n    mask_rle: run-length as string formated (start length)\n    shape: (height,width) of array to return \n    Returns numpy array, 1 - mask, 0 - background\n\n    '''\n    s = mask_rle.split()\n    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n    starts -= 1\n    ends = starts + lengths\n    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n    for lo, hi in zip(starts, ends):\n        img[lo:hi] = 1\n    return img.reshape(shape)  # Needed to align to RLE direction\n\n\n# ref.: https://www.kaggle.com/stainsby/fast-tested-rle\ndef rle_encode(img):\n    '''\n    img: numpy array, 1 - mask, 0 - background\n    Returns run length as string formated\n    '''\n    pixels = img.flatten()\n    pixels = np.concatenate([[0], pixels, [0]])\n    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n    runs[1::2] -= runs[::2]\n    return ' '.join(str(x) for x in runs)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-07-03T02:02:29.911751Z","iopub.execute_input":"2022-07-03T02:02:29.912362Z","iopub.status.idle":"2022-07-03T02:02:29.925145Z","shell.execute_reply.started":"2022-07-03T02:02:29.912326Z","shell.execute_reply":"2022-07-03T02:02:29.924344Z"},"trusted":true},"execution_count":173,"outputs":[]},{"cell_type":"markdown","source":"# 📖 Meta Data","metadata":{}},{"cell_type":"code","source":"BASE_PATH  = '/kaggle/input/uw-madison-gi-tract-image-segmentation'\nCKPT_DIR = '../input/mobilenet-3-fold'","metadata":{"execution":{"iopub.status.busy":"2022-07-03T02:57:30.628460Z","iopub.execute_input":"2022-07-03T02:57:30.628745Z","iopub.status.idle":"2022-07-03T02:57:30.633289Z","shell.execute_reply.started":"2022-07-03T02:57:30.628716Z","shell.execute_reply":"2022-07-03T02:57:30.632265Z"},"trusted":true},"execution_count":211,"outputs":[]},{"cell_type":"markdown","source":"## Train","metadata":{}},{"cell_type":"code","source":"# df = pd.read_csv('../input/uwmgi-mask-dataset/uw-madison-gi-tract-image-segmentation/train.csv')\n# df['empty'] = df.segmentation.map(lambda x: int(pd.isna(x)))\n\n# df2 = df.groupby(['id'])['class'].agg(list).to_frame().reset_index()\n# df2 = df2.merge(df.groupby(['id'])['segmentation'].agg(list), on=['id'])\n# # df = df[['id','case','day','image_path','mask_path','height','width', 'empty']]\n\n# df = df.drop(columns=['segmentation', 'class'])\n# df = df.groupby(['id']).head(1).reset_index(drop=True)\n# df = df.merge(df2, on=['id'])\n# df.head()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-07-03T02:26:46.534453Z","iopub.execute_input":"2022-07-03T02:26:46.534946Z","iopub.status.idle":"2022-07-03T02:26:46.539040Z","shell.execute_reply.started":"2022-07-03T02:26:46.534905Z","shell.execute_reply":"2022-07-03T02:26:46.538128Z"},"trusted":true},"execution_count":193,"outputs":[]},{"cell_type":"markdown","source":"## Test","metadata":{}},{"cell_type":"code","source":"sub_df = pd.read_csv('../input/uw-madison-gi-tract-image-segmentation/sample_submission.csv')\nif not len(sub_df):\n    debug = True\n    sub_df = pd.read_csv('../input/uw-madison-gi-tract-image-segmentation/train.csv')\n    sub_df = sub_df[~sub_df.segmentation.isna()][:1000*3]\n    sub_df = sub_df.drop(columns=['class','segmentation']).drop_duplicates()\nelse:\n    debug = False\n    sub_df = sub_df.drop(columns=['class','predicted']).drop_duplicates()\nsub_df = sub_df.progress_apply(get_metadata,axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-07-03T02:02:29.952093Z","iopub.execute_input":"2022-07-03T02:02:29.953579Z","iopub.status.idle":"2022-07-03T02:02:32.583958Z","shell.execute_reply.started":"2022-07-03T02:02:29.953512Z","shell.execute_reply":"2022-07-03T02:02:32.583035Z"},"trusted":true},"execution_count":176,"outputs":[{"name":"stderr","text":"100%|██████████| 1429/1429 [00:02<00:00, 612.11it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"if debug:\n    paths = glob(f'/kaggle/input/uw-madison-gi-tract-image-segmentation/train/**/*png',recursive=True)\n#     paths = sorted(paths)\nelse:\n    paths = glob(f'/kaggle/input/uw-madison-gi-tract-image-segmentation/test/**/*png',recursive=True)\n#     paths = sorted(paths)\npath_df = pd.DataFrame(paths, columns=['image_path'])\npath_df = path_df.progress_apply(path2info, axis=1)\npath_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-07-03T02:02:32.585235Z","iopub.execute_input":"2022-07-03T02:02:32.585579Z","iopub.status.idle":"2022-07-03T02:04:08.181983Z","shell.execute_reply.started":"2022-07-03T02:02:32.585541Z","shell.execute_reply":"2022-07-03T02:04:08.181264Z"},"trusted":true},"execution_count":177,"outputs":[{"name":"stderr","text":"100%|██████████| 38496/38496 [01:34<00:00, 405.93it/s]\n","output_type":"stream"},{"execution_count":177,"output_type":"execute_result","data":{"text/plain":"                                          image_path  height  width  case  \\\n0  /kaggle/input/uw-madison-gi-tract-image-segmen...     266    266    36   \n1  /kaggle/input/uw-madison-gi-tract-image-segmen...     266    266    36   \n2  /kaggle/input/uw-madison-gi-tract-image-segmen...     266    266    36   \n3  /kaggle/input/uw-madison-gi-tract-image-segmen...     266    266    36   \n4  /kaggle/input/uw-madison-gi-tract-image-segmen...     266    266    36   \n\n   day  slice  \n0   14      6  \n1   14     82  \n2   14    113  \n3   14     76  \n4   14    125  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_path</th>\n      <th>height</th>\n      <th>width</th>\n      <th>case</th>\n      <th>day</th>\n      <th>slice</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>/kaggle/input/uw-madison-gi-tract-image-segmen...</td>\n      <td>266</td>\n      <td>266</td>\n      <td>36</td>\n      <td>14</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>/kaggle/input/uw-madison-gi-tract-image-segmen...</td>\n      <td>266</td>\n      <td>266</td>\n      <td>36</td>\n      <td>14</td>\n      <td>82</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>/kaggle/input/uw-madison-gi-tract-image-segmen...</td>\n      <td>266</td>\n      <td>266</td>\n      <td>36</td>\n      <td>14</td>\n      <td>113</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>/kaggle/input/uw-madison-gi-tract-image-segmen...</td>\n      <td>266</td>\n      <td>266</td>\n      <td>36</td>\n      <td>14</td>\n      <td>76</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>/kaggle/input/uw-madison-gi-tract-image-segmen...</td>\n      <td>266</td>\n      <td>266</td>\n      <td>36</td>\n      <td>14</td>\n      <td>125</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"## Merge Data","metadata":{}},{"cell_type":"code","source":"test_df = sub_df.merge(path_df, on=['case','day','slice'], how='left')\ntest_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-07-03T02:26:55.304669Z","iopub.execute_input":"2022-07-03T02:26:55.304928Z","iopub.status.idle":"2022-07-03T02:26:55.331939Z","shell.execute_reply.started":"2022-07-03T02:26:55.304901Z","shell.execute_reply":"2022-07-03T02:26:55.331157Z"},"trusted":true},"execution_count":194,"outputs":[{"execution_count":194,"output_type":"execute_result","data":{"text/plain":"                         id  case  day  slice  \\\n0  case123_day20_slice_0065   123   20     65   \n1  case123_day20_slice_0066   123   20     66   \n2  case123_day20_slice_0067   123   20     67   \n3  case123_day20_slice_0068   123   20     68   \n4  case123_day20_slice_0069   123   20     69   \n\n                                          image_path  height  width  \n0  /kaggle/input/uw-madison-gi-tract-image-segmen...     266    266  \n1  /kaggle/input/uw-madison-gi-tract-image-segmen...     266    266  \n2  /kaggle/input/uw-madison-gi-tract-image-segmen...     266    266  \n3  /kaggle/input/uw-madison-gi-tract-image-segmen...     266    266  \n4  /kaggle/input/uw-madison-gi-tract-image-segmen...     266    266  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>case</th>\n      <th>day</th>\n      <th>slice</th>\n      <th>image_path</th>\n      <th>height</th>\n      <th>width</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>case123_day20_slice_0065</td>\n      <td>123</td>\n      <td>20</td>\n      <td>65</td>\n      <td>/kaggle/input/uw-madison-gi-tract-image-segmen...</td>\n      <td>266</td>\n      <td>266</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>case123_day20_slice_0066</td>\n      <td>123</td>\n      <td>20</td>\n      <td>66</td>\n      <td>/kaggle/input/uw-madison-gi-tract-image-segmen...</td>\n      <td>266</td>\n      <td>266</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>case123_day20_slice_0067</td>\n      <td>123</td>\n      <td>20</td>\n      <td>67</td>\n      <td>/kaggle/input/uw-madison-gi-tract-image-segmen...</td>\n      <td>266</td>\n      <td>266</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>case123_day20_slice_0068</td>\n      <td>123</td>\n      <td>20</td>\n      <td>68</td>\n      <td>/kaggle/input/uw-madison-gi-tract-image-segmen...</td>\n      <td>266</td>\n      <td>266</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>case123_day20_slice_0069</td>\n      <td>123</td>\n      <td>20</td>\n      <td>69</td>\n      <td>/kaggle/input/uw-madison-gi-tract-image-segmen...</td>\n      <td>266</td>\n      <td>266</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"## 2.5D MetaData","metadata":{}},{"cell_type":"code","source":"channels=3\nstride=2\nfor i in range(channels):\n    test_df[f'image_path_{i:02}'] = test_df.groupby(['case','day'])['image_path'].shift(-i*stride).fillna(method=\"ffill\")\ntest_df['image_paths'] = test_df[[f'image_path_{i:02d}' for i in range(channels)]].values.tolist()\nif debug:\n    test_df = test_df.sample(frac=1.0)\ntest_df.image_paths[0]","metadata":{"execution":{"iopub.status.busy":"2022-07-03T02:58:19.630679Z","iopub.execute_input":"2022-07-03T02:58:19.630937Z","iopub.status.idle":"2022-07-03T02:58:19.651326Z","shell.execute_reply.started":"2022-07-03T02:58:19.630909Z","shell.execute_reply":"2022-07-03T02:58:19.650356Z"},"trusted":true},"execution_count":212,"outputs":[{"execution_count":212,"output_type":"execute_result","data":{"text/plain":"['/kaggle/input/uw-madison-gi-tract-image-segmentation/train/case123/case123_day20/scans/slice_0065_266_266_1.50_1.50.png',\n '/kaggle/input/uw-madison-gi-tract-image-segmentation/train/case123/case123_day20/scans/slice_0099_266_266_1.50_1.50.png',\n '/kaggle/input/uw-madison-gi-tract-image-segmentation/train/case123/case123_day20/scans/slice_0106_266_266_1.50_1.50.png']"},"metadata":{}}]},{"cell_type":"markdown","source":"# 🍚 Dataset","metadata":{}},{"cell_type":"code","source":"class BuildDataset(torch.utils.data.Dataset):\n    def __init__(self, df, label=False, transforms=None):\n        self.df         = df\n        self.label      = label\n        self.img_paths  = df['image_paths'].tolist()\n        self.ids        = df['id'].tolist()\n        if 'msk_path' in df.columns:\n            self.msk_paths  = df['mask_path'].tolist()\n        else:\n            self.msk_paths = None\n        self.transforms = transforms\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, index):\n        img_path  = self.img_paths[index]\n        id_       = self.ids[index]\n        img = []\n        img, shape0 = load_imgs(img_path)\n        h, w = shape0\n        if self.label:\n            msk_path = self.msk_paths[index]\n            msk = load_msk(msk_path)\n            if self.transforms:\n                data = self.transforms(image=img, mask=msk)\n                img  = data['image']\n                msk  = data['mask']\n            img = np.transpose(img, (2, 0, 1))\n            msk = np.transpose(msk, (2, 0, 1))\n            return torch.tensor(img), torch.tensor(msk)\n        else:\n            if self.transforms:\n                data = self.transforms(image=img)\n                img  = data['image']\n            img = np.transpose(img, (2, 0, 1))\n            return torch.tensor(img), id_, h, w","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-07-03T02:58:20.337789Z","iopub.execute_input":"2022-07-03T02:58:20.338042Z","iopub.status.idle":"2022-07-03T02:58:20.349341Z","shell.execute_reply.started":"2022-07-03T02:58:20.338015Z","shell.execute_reply":"2022-07-03T02:58:20.348297Z"},"trusted":true},"execution_count":213,"outputs":[]},{"cell_type":"markdown","source":"# 🌈 Augmentations","metadata":{}},{"cell_type":"code","source":"data_transforms = {\n    \"train\": A.Compose([\n#         A.Resize(*CFG.img_size, interpolation=cv2.INTER_NEAREST),\n        A.HorizontalFlip(p=0.5),\n        A.VerticalFlip(p=0.5),\n#         A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.05, rotate_limit=5, p=0.5),\n        A.OneOf([\n            A.GridDistortion(num_steps=5, distort_limit=0.05, p=1.0),\n# #             A.OpticalDistortion(distort_limit=0.05, shift_limit=0.05, p=1.0),\n            A.ElasticTransform(alpha=1, sigma=50, alpha_affine=50, p=1.0)\n        ], p=0.25),\n#         A.CoarseDropout(max_holes=8, max_height=CFG.img_size[0]//20, max_width=CFG.img_size[1]//20,\n#                          min_holes=5, fill_value=0, mask_fill_value=0, p=0.5),\n        ], p=1.0),\n    \n    \"valid\": A.Compose([\n#         A.Resize(*CFG.img_size, interpolation=cv2.INTER_NEAREST),\n        ], p=1.0)\n}","metadata":{"_kg_hide-output":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-07-03T02:58:20.760636Z","iopub.execute_input":"2022-07-03T02:58:20.761339Z","iopub.status.idle":"2022-07-03T02:58:20.768057Z","shell.execute_reply.started":"2022-07-03T02:58:20.761292Z","shell.execute_reply":"2022-07-03T02:58:20.767301Z"},"trusted":true},"execution_count":214,"outputs":[]},{"cell_type":"markdown","source":"# 🍰 DataLoader","metadata":{}},{"cell_type":"code","source":"# test_dataset = BuildDataset(test_df, transforms=data_transforms['valid'])\n# test_loader  = DataLoader(test_dataset, batch_size=64, \n#                           num_workers=4, shuffle=False, pin_memory=True)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-07-03T02:58:21.200135Z","iopub.execute_input":"2022-07-03T02:58:21.200666Z","iopub.status.idle":"2022-07-03T02:58:21.204621Z","shell.execute_reply.started":"2022-07-03T02:58:21.200630Z","shell.execute_reply":"2022-07-03T02:58:21.203862Z"},"trusted":true},"execution_count":215,"outputs":[]},{"cell_type":"code","source":"# imgs, ids, (h, w) = next(iter(test_loader))\n# imgs = imgs.permute((0, 2, 3, 1))\n# imgs.size()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-07-03T02:58:21.530902Z","iopub.execute_input":"2022-07-03T02:58:21.531149Z","iopub.status.idle":"2022-07-03T02:58:21.535537Z","shell.execute_reply.started":"2022-07-03T02:58:21.531121Z","shell.execute_reply":"2022-07-03T02:58:21.534445Z"},"trusted":true},"execution_count":216,"outputs":[]},{"cell_type":"markdown","source":"# 📦 Model\n","metadata":{}},{"cell_type":"markdown","source":"## UNet\n\n<img src=\"https://developers.arcgis.com/assets/img/python-graphics/unet.png\" width=\"600\">\n\n📌 **Pros**:\n* Performs well even with smaller data\n* Can be used with `imagenet` pretrain models\n\n📌 **Cons**:\n* Struggles with **edge** cases\n* Semantic Difference in **Skip Connection**","metadata":{}},{"cell_type":"code","source":"import segmentation_models_pytorch as smp\n\ndef build_model():\n    model = smp.Unet(\n        encoder_name=CFG.backbone,      # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n        encoder_weights=None,     # use `imagenet` pre-trained weights for encoder initialization\n        in_channels=3,                  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n        classes=CFG.num_classes,        # model output channels (number of classes in your dataset)\n        activation=None,\n    )\n    model.to(CFG.device)\n    return model\n\ndef load_model(path):\n    model = build_model()\n    model.load_state_dict(torch.load(path))\n    model.eval()\n    return model","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-07-03T02:58:22.200053Z","iopub.execute_input":"2022-07-03T02:58:22.200607Z","iopub.status.idle":"2022-07-03T02:58:22.206899Z","shell.execute_reply.started":"2022-07-03T02:58:22.200571Z","shell.execute_reply":"2022-07-03T02:58:22.205853Z"},"trusted":true},"execution_count":217,"outputs":[]},{"cell_type":"code","source":"# # test\n# img = torch.randn(1, 1, *CFG.img_size).to(CFG.device)\n# img = (img - img.min())/(img.max() - img.min())\n# model = build_model()\n# _ = model(img)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-07-03T02:58:22.291617Z","iopub.execute_input":"2022-07-03T02:58:22.292228Z","iopub.status.idle":"2022-07-03T02:58:22.295564Z","shell.execute_reply.started":"2022-07-03T02:58:22.292190Z","shell.execute_reply":"2022-07-03T02:58:22.294765Z"},"trusted":true},"execution_count":218,"outputs":[]},{"cell_type":"markdown","source":"# 🔨 Helper","metadata":{}},{"cell_type":"code","source":"import cupy as cp\n\ndef mask2rle(msk, thr=0.5):\n    '''\n    img: numpy array, 1 - mask, 0 - background\n    Returns run length as string formated\n    '''\n    msk    = cp.array(msk)\n    pixels = msk.flatten()\n    pad    = cp.array([0])\n    pixels = cp.concatenate([pad, pixels, pad])\n    runs   = cp.where(pixels[1:] != pixels[:-1])[0] + 1\n    runs[1::2] -= runs[::2]\n    return ' '.join(str(x) for x in runs)\n\ndef masks2rles(msks, ids, heights, widths):\n    pred_strings = []; pred_ids = []; pred_classes = [];\n    for idx in range(msks.shape[0]):\n        msk = msks[idx]\n        height = heights[idx].item()\n        width = widths[idx].item()\n        shape0 = np.array([height, width])\n        resize = np.array([320, 384])\n        if np.any(shape0!=resize):\n            diff = resize - shape0\n            pad0 = diff[0]\n            pad1 = diff[1]\n            pady = [pad0//2, pad0//2 + pad0%2]\n            padx = [pad1//2, pad1//2 + pad1%2]\n            msk = msk[pady[0]:-pady[1], padx[0]:-padx[1], :]\n            msk = msk.reshape((*shape0, 3))\n        rle = [None]*3\n        for midx in [0, 1, 2]:\n            rle[midx] = mask2rle(msk[...,midx])\n        pred_strings.extend(rle)\n        pred_ids.extend([ids[idx]]*len(rle))\n        pred_classes.extend(['large_bowel', 'small_bowel', 'stomach'])\n    return pred_strings, pred_ids, pred_classes","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-07-03T02:59:33.348958Z","iopub.execute_input":"2022-07-03T02:59:33.349735Z","iopub.status.idle":"2022-07-03T02:59:33.362730Z","shell.execute_reply.started":"2022-07-03T02:59:33.349691Z","shell.execute_reply":"2022-07-03T02:59:33.361624Z"},"trusted":true},"execution_count":223,"outputs":[]},{"cell_type":"markdown","source":"# 🔭 Inference","metadata":{}},{"cell_type":"code","source":"@torch.no_grad()\ndef infer(model_paths, test_loader, num_log=1, thr=CFG.thr):\n    msks = []; imgs = [];\n    pred_strings = []; pred_ids = []; pred_classes = [];\n    for idx, (img, ids, heights, widths) in enumerate(tqdm(test_loader, total=len(test_loader), desc='Infer ')):\n        img = img.to(CFG.device, dtype=torch.float) # .squeeze(0)\n        size = img.size()\n        msk = []\n        msk = torch.zeros((size[0], 3, size[2], size[3]), device=CFG.device, dtype=torch.float32)\n        for path in model_paths:\n            model = load_model(path)\n            out   = model(img) # .squeeze(0) # removing batch axis\n            out   = nn.Sigmoid()(out) # removing channel axis\n            msk+=out/len(model_paths)\n        msk = (msk.permute((0,2,3,1))>thr).to(torch.uint8).cpu().detach().numpy() # shape: (n, h, w, c)\n        result = masks2rles(msk, ids, heights, widths)\n        pred_strings.extend(result[0])\n        pred_ids.extend(result[1])\n        pred_classes.extend(result[2])\n        if idx<num_log and debug:\n            img = img.permute((0,2,3,1)).cpu().detach().numpy()\n            imgs.append(img[::5])\n            msks.append(msk[::5])\n        del img, msk, out, model, result\n        gc.collect()\n        torch.cuda.empty_cache()\n    return pred_strings, pred_ids, pred_classes, imgs, msks","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-07-03T02:59:34.959020Z","iopub.execute_input":"2022-07-03T02:59:34.959557Z","iopub.status.idle":"2022-07-03T02:59:34.971152Z","shell.execute_reply.started":"2022-07-03T02:59:34.959523Z","shell.execute_reply":"2022-07-03T02:59:34.970076Z"},"trusted":true},"execution_count":224,"outputs":[]},{"cell_type":"code","source":"print(CKPT_DIR)","metadata":{"execution":{"iopub.status.busy":"2022-07-03T02:59:36.247792Z","iopub.execute_input":"2022-07-03T02:59:36.248044Z","iopub.status.idle":"2022-07-03T02:59:36.252683Z","shell.execute_reply.started":"2022-07-03T02:59:36.248016Z","shell.execute_reply":"2022-07-03T02:59:36.251925Z"},"trusted":true},"execution_count":225,"outputs":[{"name":"stdout","text":"../input/mobilenet-3-fold\n","output_type":"stream"}]},{"cell_type":"code","source":"test_dataset = BuildDataset(test_df, transforms=data_transforms['valid'])\ntest_loader  = DataLoader(test_dataset, batch_size=CFG.valid_bs, \n                          num_workers=4, shuffle=False, pin_memory=False)\nmodel_paths  = glob(f\"../input/mobilenet-3-fold/best_epoch-00.bin\")\npred_strings, pred_ids, pred_classes, imgs, msks = infer(model_paths, test_loader)","metadata":{"execution":{"iopub.status.busy":"2022-07-03T02:59:36.856652Z","iopub.execute_input":"2022-07-03T02:59:36.856972Z","iopub.status.idle":"2022-07-03T02:59:38.779657Z","shell.execute_reply.started":"2022-07-03T02:59:36.856938Z","shell.execute_reply":"2022-07-03T02:59:38.777448Z"},"trusted":true},"execution_count":226,"outputs":[{"name":"stderr","text":"Infer :   0%|          | 0/90 [00:01<?, ?it/s]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_34/2002478669.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m                           num_workers=4, shuffle=False, pin_memory=False)\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel_paths\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"../input/mobilenet-3-fold/best_epoch-00.bin\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mpred_strings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minfer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_paths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_34/2806413700.py\u001b[0m in \u001b[0;36minfer\u001b[0;34m(model_paths, test_loader, num_log, thr)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mmsk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCFG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel_paths\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m             \u001b[0mout\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# .squeeze(0) # removing batch axis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mout\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# removing channel axis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_34/471832069.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1405\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1406\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0;32m-> 1407\u001b[0;31m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[1;32m   1408\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for Unet:\n\tMissing key(s) in state_dict: \"encoder.model.conv_stem.weight\", \"encoder.model.bn1.weight\", \"encoder.model.bn1.bias\", \"encoder.model.bn1.running_mean\", \"encoder.model.bn1.running_var\", \"encoder.model.blocks.0.0.conv_dw.weight\", \"encoder.model.blocks.0.0.bn1.weight\", \"encoder.model.blocks.0.0.bn1.bias\", \"encoder.model.blocks.0.0.bn1.running_mean\", \"encoder.model.blocks.0.0.bn1.running_var\", \"encoder.model.blocks.0.0.conv_pw.weight\", \"encoder.model.blocks.0.0.bn2.weight\", \"encoder.model.blocks.0.0.bn2.bias\", \"encoder.model.blocks.0.0.bn2.running_mean\", \"encoder.model.blocks.0.0.bn2.running_var\", \"encoder.model.blocks.1.0.conv_pw.weight\", \"encoder.model.blocks.1.0.bn1.weight\", \"encoder.model.blocks.1.0.bn1.bias\", \"encoder.model.blocks.1.0.bn1.running_mean\", \"encoder.model.blocks.1.0.bn1.running_var\", \"encoder.model.blocks.1.0.conv_dw.weight\", \"encoder.model.blocks.1.0.bn2.weight\", \"encoder.model.blocks.1.0.bn2.bias\", \"encoder.model.blocks.1.0.bn2.running_mean\", \"encoder.model.blocks.1.0.bn2.running_var\", \"encoder.model.blocks.1.0.conv_pwl.weight\", \"encoder.model.blocks.1.0.bn3.weight\", \"encoder.model.blocks.1.0.bn3.bias\", \"encoder.model.blocks.1.0.bn3.running_mean\", \"encoder.model.blocks.1.0.bn3.running_var\", \"encoder.model.blocks.1.1.conv_pw.weight\", \"encoder.model.blocks.1.1.bn1.weight\", \"encoder.model.blocks.1.1.bn1.bias\", \"encoder.model.blocks.1.1.bn1.running_mean\", \"encoder.model.blocks.1.1.bn1.running_var\", \"encoder.model.blocks.1.1.conv_dw.weight\", \"encoder.model.blocks.1.1.bn2.weight\", \"encoder.model.blocks.1.1.bn2.bias\", \"encoder.model.blocks.1.1.bn2.running_mean\", \"encoder.model.blocks.1.1.bn2.running_var\", \"encoder.model.blocks.1.1.conv_pwl.weight\", \"encoder.model.blocks.1.1.bn3.weight\", \"encoder.model.blocks.1.1.bn3.bias\", \"encoder.model.blocks.1.1.bn3.running_mean\", \"encoder.model.blocks.1.1.bn3.running_var\", \"encoder.model.blocks.2.0.conv_pw.weight\", \"encoder.model.blocks.2.0.bn1.weight\", \"encoder.model.blocks.2.0.bn1.bias\", \"encoder.model.blocks.2.0.bn1.running_mean\", \"encoder.model.blocks.2.0.bn1.running_var\", \"encoder.model.blocks.2.0.conv_dw.weight\", \"encoder.model.blocks.2.0.bn2.weight\", \"encoder.model.blocks.2.0.bn2.bias\", \"encoder.model.blocks.2.0.bn2.running_mean\", \"encoder.model.blocks.2.0.bn2.running_var\", \"encoder.model.blocks.2.0.se.conv_reduce.weight\", \"encoder.model.blocks.2.0.se.conv_reduce.bias\", \"encoder.model.blocks.2.0.se.conv_expand.weight\", \"encoder.model.blocks.2.0.se.conv_expand.bias\", \"encoder.model.blocks.2.0.conv_pwl.weight\", \"encoder.model.blocks.2.0.bn3.weight\", \"encoder.model.blocks.2.0.bn3.bias\", \"encoder.model.blocks.2.0.bn3.running_mean\", \"encoder.model.blocks.2.0.bn3.running_var\", \"encoder.model.blocks.2.1.conv_pw.weight\", \"encoder.model.blocks.2.1.bn1.weight\", \"encoder.model.blocks.2.1.bn1.bias\", \"encoder.model.blocks.2.1.bn1.running_mean\", \"encoder.model.blocks.2.1.bn1.running_var\", \"encoder.model.blocks.2.1.conv_dw.weight\", \"encoder.model.blocks.2.1.bn2.weight\", \"encoder.model.blocks.2.1.bn2.bias\", \"encoder.model.blocks.2.1.bn2.running_mean\", \"encoder.model.blocks.2.1.bn2.running_var\", \"encoder.model.blocks.2.1.se.conv_reduce.weight\", \"encoder.model.blocks.2.1.se.conv_reduce.bias\", \"encoder.model.blocks.2.1.se.conv_expand.weight\", \"encoder.model.blocks.2.1.se.conv_expand.bias\", \"encoder.model.blocks.2.1.conv_pwl.weight\", \"encoder.model.blocks.2.1.bn3.weight\", \"encoder.model.blocks.2.1.bn3.bias\", \"encoder.model.blocks.2.1.bn3.running_mean\", \"encoder.model.blocks.2.1.bn3.running_var\", \"encoder.model.blocks.2.2.conv_pw.weight\", \"encoder.model.blocks.2.2.bn1.weight\", \"encoder.model.blocks.2.2.bn1.bias\", \"encoder.model.blocks.2.2.bn1.running_mean\", \"encoder.model.blocks.2.2.bn1.running_var\", \"encoder.model.blocks.2.2.conv_dw.weight\", \"encoder.model.blocks.2.2.bn2.weight\", \"encoder.model.blocks.2.2.bn2.bias\", \"encoder.model.blocks.2.2.bn2.running_mean\", \"encoder.model.blocks.2.2.bn2.running_var\", \"encoder.model.blocks.2.2.se.conv_reduce.weight\", \"encoder.model.blocks.2.2.se.conv_reduce.bias\", \"encoder.model.blocks.2.2.se.conv_expand.weight\", \"encoder.model.blocks.2.2.se.conv_expand.bias\", \"encoder.model.blocks.2.2.conv_pwl.weight\", \"encoder.model.blocks.2.2.bn3.weight\", \"encoder.model.blocks.2.2.bn3.bias\", \"encoder.model.blocks.2.2.bn3.running_mean\", \"encoder.model.blocks.2.2.bn3.running_var\", \"encoder.model.blocks.3.0.conv_pw.weight\", \"encoder.model.blocks.3.0.bn1.weight\", \"encoder.model.blocks.3.0.bn1.bias\", \"encoder.model.blocks.3.0.bn1.running_mean\", \"encoder.model.blocks.3.0.bn1.running_var\", \"encoder.model.blocks.3.0.conv_dw.weight\", \"encoder.model.blocks.3.0.bn2.weight\", \"encoder.model.blocks.3.0.bn2.bias\", \"encoder.model.blocks.3.0.bn2.running_mean\", \"encoder.model.blocks.3.0.bn2.running_var\", \"encoder.model.blocks.3.0.conv_pwl.weight\", \"encoder.model.blocks.3.0.bn3.weight\", \"encoder.model.blocks.3.0.bn3.bias\", \"encoder.model.blocks.3.0.bn3.running_mean\", \"encoder.model.blocks.3.0.bn3.running_var\", \"encoder.model.blocks.3.1.conv_pw.weight\", \"encoder.model.blocks.3.1.bn1.weight\", \"encoder.model.blocks.3.1.bn1.bias\", \"encoder.model.blocks.3.1.bn1.running_mean\", \"encoder.model.blocks.3.1.bn1.running_var\", \"encoder.model.blocks.3.1.conv_dw.weight\", \"encoder.model.blocks.3.1.bn2.weight\", \"encoder.model.blocks.3.1.bn2.bias\", \"encoder.model.blocks.3.1.bn2.running_mean\", \"encoder.model.blocks.3.1.bn2.running_var\", \"encoder.model.blocks.3.1.conv_pwl.weight\", \"encoder.model.blocks.3.1.bn3.weight\", \"encoder.model.blocks.3.1.bn3.bias\", \"encoder.model.blocks.3.1.bn3.running_mean\", \"encoder.model.blocks.3.1.bn3.running_var\", \"encoder.model.blocks.3.2.conv_pw.weight\", \"encoder.model.blocks.3.2.bn1.weight\", \"encoder.model.blocks.3.2.bn1.bias\", \"encoder.model.blocks.3.2.bn1.running_mean\", \"encoder.model.blocks.3.2.bn1.running_var\", \"encoder.model.blocks.3.2.conv_dw.weight\", \"encoder.model.blocks.3.2.bn2.weight\", \"encoder.model.blocks.3.2.bn2.bias\", \"encoder.model.blocks.3.2.bn2.running_mean\", \"encoder.model.blocks.3.2.bn2.running_var\", \"encoder.model.blocks.3.2.conv_pwl.weight\", \"encoder.model.blocks.3.2.bn3.weight\", \"encoder.model.blocks.3.2.bn3.bias\", \"encoder.model.blocks.3.2.bn3.running_mean\", \"encoder.model.blocks.3.2.bn3.running_var\", \"encoder.model.blocks.3.3.conv_pw.weight\", \"encoder.model.blocks.3.3.bn1.weight\", \"encoder.model.blocks.3.3.bn1.bias\", \"encoder.model.blocks.3.3.bn1.running_mean\", \"encoder.model.blocks.3.3.bn1.running_var\", \"encoder.model.blocks.3.3.conv_dw.weight\", \"encoder.model.blocks.3.3.bn2.weight\", \"encoder.model.blocks.3.3.bn2.bias\", \"encoder.model.blocks.3.3.bn2.running_mean\", \"encoder.model.blocks.3.3.bn2.running_var\", \"encoder.model.blocks.3.3.conv_pwl.weight\", \"encoder.model.blocks.3.3.bn3.weight\", \"encoder.model.blocks.3.3.bn3.bias\", \"encoder.model.blocks.3.3.bn3.running_mean\", \"encoder.model.blocks.3.3.bn3.running_var\", \"encoder.model.blocks.4.0.conv_pw.weight\", \"encoder.model.blocks.4.0.bn1.weight\", \"encoder.model.blocks.4.0.bn1.bias\", \"encoder.model.blocks.4.0.bn1.running_mean\", \"encoder.model.blocks.4.0.bn1.running_var\", \"encoder.model.blocks.4.0.conv_dw.weight\", \"encoder.model.blocks.4.0.bn2.weight\", \"encoder.model.blocks.4.0.bn2.bias\", \"encoder.model.blocks.4.0.bn2.running_mean\", \"encoder.model.blocks.4.0.bn2.running_var\", \"encoder.model.blocks.4.0.se.conv_reduce.weight\", \"encoder.model.blocks.4.0.se.conv_reduce.bias\", \"encoder.model.blocks.4.0.se.conv_expand.weight\", \"encoder.model.blocks.4.0.se.conv_expand.bias\", \"encoder.model.blocks.4.0.conv_pwl.weight\", \"encoder.model.blocks.4.0.bn3.weight\", \"encoder.model.blocks.4.0.bn3.bias\", \"encoder.model.blocks.4.0.bn3.running_mean\", \"encoder.model.blocks.4.0.bn3.running_var\", \"encoder.model.blocks.4.1.conv_pw.weight\", \"encoder.model.blocks.4.1.bn1.weight\", \"encoder.model.blocks.4.1.bn1.bias\", \"encoder.model.blocks.4.1.bn1.running_mean\", \"encoder.model.blocks.4.1.bn1.running_var\", \"encoder.model.blocks.4.1.conv_dw.weight\", \"encoder.model.blocks.4.1.bn2.weight\", \"encoder.model.blocks.4.1.bn2.bias\", \"encoder.model.blocks.4.1.bn2.running_mean\", \"encoder.model.blocks.4.1.bn2.running_var\", \"encoder.model.blocks.4.1.se.conv_reduce.weight\", \"encoder.model.blocks.4.1.se.conv_reduce.bias\", \"encoder.model.blocks.4.1.se.conv_expand.weight\", \"encoder.model.blocks.4.1.se.conv_expand.bias\", \"encoder.model.blocks.4.1.conv_pwl.weight\", \"encoder.model.blocks.4.1.bn3.weight\", \"encoder.model.blocks.4.1.bn3.bias\", \"encoder.model.blocks.4.1.bn3.running_mean\", \"encoder.model.blocks.4.1.bn3.running_var\", \"encoder.model.blocks.5.0.conv_pw.weight\", \"encoder.model.blocks.5.0.bn1.weight\", \"encoder.model.blocks.5.0.bn1.bias\", \"encoder.model.blocks.5.0.bn1.running_mean\", \"encoder.model.blocks.5.0.bn1.running_var\", \"encoder.model.blocks.5.0.conv_dw.weight\", \"encoder.model.blocks.5.0.bn2.weight\", \"encoder.model.blocks.5.0.bn2.bias\", \"encoder.model.blocks.5.0.bn2.running_mean\", \"encoder.model.blocks.5.0.bn2.running_var\", \"encoder.model.blocks.5.0.se.conv_reduce.weight\", \"encoder.model.blocks.5.0.se.conv_reduce.bias\", \"encoder.model.blocks.5.0.se.conv_expand.weight\", \"encoder.model.blocks.5.0.se.conv_expand.bias\", \"encoder.model.blocks.5.0.conv_pwl.weight\", \"encoder.model.blocks.5.0.bn3.weight\", \"encoder.model.blocks.5.0.bn3.bias\", \"encoder.model.blocks.5.0.bn3.running_mean\", \"encoder.model.blocks.5.0.bn3.running_var\", \"encoder.model.blocks.5.1.conv_pw.weight\", \"encoder.model.blocks.5.1.bn1.weight\", \"encoder.model.blocks.5.1.bn1.bias\", \"encoder.model.blocks.5.1.bn1.running_mean\", \"encoder.model.blocks.5.1.bn1.running_var\", \"encoder.model.blocks.5.1.conv_dw.weight\", \"encoder.model.blocks.5.1.bn2.weight\", \"encoder.model.blocks.5.1.bn2.bias\", \"encoder.model.blocks.5.1.bn2.running_mean\", \"encoder.model.blocks.5.1.bn2.running_var\", \"encoder.model.blocks.5.1.se.conv_reduce.weight\", \"encoder.model.blocks.5.1.se.conv_reduce.bias\", \"encoder.model.blocks.5.1.se.conv_expand.weight\", \"encoder.model.blocks.5.1.se.conv_expand.bias\", \"encoder.model.blocks.5.1.conv_pwl.weight\", \"encoder.model.blocks.5.1.bn3.weight\", \"encoder.model.blocks.5.1.bn3.bias\", \"encoder.model.blocks.5.1.bn3.running_mean\", \"encoder.model.blocks.5.1.bn3.running_var\", \"encoder.model.blocks.5.2.conv_pw.weight\", \"encoder.model.blocks.5.2.bn1.weight\", \"encoder.model.blocks.5.2.bn1.bias\", \"encoder.model.blocks.5.2.bn1.running_mean\", \"encoder.model.blocks.5.2.bn1.running_var\", \"encoder.model.blocks.5.2.conv_dw.weight\", \"encoder.model.blocks.5.2.bn2.weight\", \"encoder.model.blocks.5.2.bn2.bias\", \"encoder.model.blocks.5.2.bn2.running_mean\", \"encoder.model.blocks.5.2.bn2.running_var\", \"encoder.model.blocks.5.2.se.conv_reduce.weight\", \"encoder.model.blocks.5.2.se.conv_reduce.bias\", \"encoder.model.blocks.5.2.se.conv_expand.weight\", \"encoder.model.blocks.5.2.se.conv_expand.bias\", \"encoder.model.blocks.5.2.conv_pwl.weight\", \"encoder.model.blocks.5.2.bn3.weight\", \"encoder.model.blocks.5.2.bn3.bias\", \"encoder.model.blocks.5.2.bn3.running_mean\", \"encoder.model.blocks.5.2.bn3.running_var\", \"encoder.model.blocks.6.0.conv.weight\", \"encoder.model.blocks.6.0.bn1.weight\", \"encoder.model.blocks.6.0.bn1.bias\", \"encoder.model.blocks.6.0.bn1.running_mean\", \"encoder.model.blocks.6.0.bn1.running_var\". \n\tUnexpected key(s) in state_dict: \"encoder.stem.conv.weight\", \"encoder.stem.bn.weight\", \"encoder.stem.bn.bias\", \"encoder.stem.bn.running_mean\", \"encoder.stem.bn.running_var\", \"encoder.stem.bn.num_batches_tracked\", \"encoder.stages.0.0.shortcut.conv.weight\", \"encoder.stages.0.0.shortcut.bn.weight\", \"encoder.stages.0.0.shortcut.bn.bias\", \"encoder.stages.0.0.shortcut.bn.running_mean\", \"encoder.stages.0.0.shortcut.bn.running_var\", \"encoder.stages.0.0.shortcut.bn.num_batches_tracked\", \"encoder.stages.0.0.conv1_kxk.conv.weight\", \"encoder.stages.0.0.conv1_kxk.bn.weight\", \"encoder.stages.0.0.conv1_kxk.bn.bias\", \"encoder.stages.0.0.conv1_kxk.bn.running_mean\", \"encoder.stages.0.0.conv1_kxk.bn.running_var\", \"encoder.stages.0.0.conv1_kxk.bn.num_batches_tracked\", \"encoder.stages.0.0.conv2_kxk.conv.weight\", \"encoder.stages.0.0.conv2_kxk.bn.weight\", \"encoder.stages.0.0.conv2_kxk.bn.bias\", \"encoder.stages.0.0.conv2_kxk.bn.running_mean\", \"encoder.stages.0.0.conv2_kxk.bn.running_var\", \"encoder.stages.0.0.conv2_kxk.bn.num_batches_tracked\", \"encoder.stages.1.0.shortcut.conv.weight\", \"encoder.stages.1.0.shortcut.bn.weight\", \"encoder.stages.1.0.shortcut.bn.bias\", \"encoder.stages.1.0.shortcut.bn.running_mean\", \"encoder.stages.1.0.shortcut.bn.running_var\", \"encoder.stages.1.0.shortcut.bn.num_batches_tracked\", \"encoder.stages.1.0.conv1_kxk.conv.weight\", \"encoder.stages.1.0.conv1_kxk.bn.weight\", \"encoder.stages.1.0.conv1_kxk.bn.bias\", \"encoder.stages.1.0.conv1_kxk.bn.running_mean\", \"encoder.stages.1.0.conv1_kxk.bn.running_var\", \"encoder.stages.1.0.conv1_kxk.bn.num_batches_tracked\", \"encoder.stages.1.0.conv2_kxk.conv.weight\", \"encoder.stages.1.0.conv2_kxk.bn.weight\", \"encoder.stages.1.0.conv2_kxk.bn.bias\", \"encoder.stages.1.0.conv2_kxk.bn.running_mean\", \"encoder.stages.1.0.conv2_kxk.bn.running_var\", \"encoder.stages.1.0.conv2_kxk.bn.num_batches_tracked\", \"encoder.stages.1.1.conv1_kxk.conv.weight\", \"encoder.stages.1.1.conv1_kxk.bn.weight\", \"encoder.stages.1.1.conv1_kxk.bn.bias\", \"encoder.stages.1.1.conv1_kxk.bn.running_mean\", \"encoder.stages.1.1.conv1_kxk.bn.running_var\", \"encoder.stages.1.1.conv1_kxk.bn.num_batches_tracked\", \"encoder.stages.1.1.conv2_kxk.conv.weight\", \"encoder.stages.1.1.conv2_kxk.bn.weight\", \"encoder.stages.1.1.conv2_kxk.bn.bias\", \"encoder.stages.1.1.conv2_kxk.bn.running_mean\", \"encoder.stages.1.1.conv2_kxk.bn.running_var\", \"encoder.stages.1.1.conv2_kxk.bn.num_batches_tracked\", \"encoder.stages.2.0.shortcut.conv.weight\", \"encoder.stages.2.0.shortcut.bn.weight\", \"encoder.stages.2.0.shortcut.bn.bias\", \"encoder.stages.2.0.shortcut.bn.running_mean\", \"encoder.stages.2.0.shortcut.bn.running_var\", \"encoder.stages.2.0.shortcut.bn.num_batches_tracked\", \"encoder.stages.2.0.conv1_1x1.conv.weight\", \"encoder.stages.2.0.conv1_1x1.bn.weight\", \"encoder.stages.2.0.conv1_1x1.bn.bias\", \"encoder.stages.2.0.conv1_1x1.bn.running_mean\", \"encoder.stages.2.0.conv1_1x1.bn.running_var\", \"encoder.stages.2.0.conv1_1x1.bn.num_batches_tracked\", \"encoder.stages.2.0.conv2_kxk.conv.weight\", \"encoder.stages.2.0.conv2_kxk.bn.weight\", \"encoder.stages.2.0.conv2_kxk.bn.bias\", \"encoder.stages.2.0.conv2_kxk.bn.running_mean\", \"encoder.stages.2.0.conv2_kxk.bn.running_var\", \"encoder.stages.2.0.conv2_kxk.bn.num_batches_tracked\", \"encoder.stages.2.0.conv3_1x1.conv.weight\", \"encoder.stages.2.0.conv3_1x1.bn.weight\", \"encoder.stages.2.0.conv3_1x1.bn.bias\", \"encoder.stages.2.0.conv3_1x1.bn.running_mean\", \"encoder.stages.2.0.conv3_1x1.bn.running_var\", \"encoder.stages.2.0.conv3_1x1.bn.num_batches_tracked\", \"encoder.stages.2.1.conv1_1x1.conv.weight\", \"encoder.stages.2.1.conv1_1x1.bn.weight\", \"encoder.stages.2.1.conv1_1x1.bn.bias\", \"encoder.stages.2.1.conv1_1x1.bn.running_mean\", \"encoder.stages.2.1.conv1_1x1.bn.running_var\", \"encoder.stages.2.1.conv1_1x1.bn.num_batches_tracked\", \"encoder.stages.2.1.conv2_kxk.conv.weight\", \"encoder.stages.2.1.conv2_kxk.bn.weight\", \"encoder.stages.2.1.conv2_kxk.bn.bias\", \"encoder.stages.2.1.conv2_kxk.bn.running_mean\", \"encoder.stages.2.1.conv2_kxk.bn.running_var\", \"encoder.stages.2.1.conv2_kxk.bn.num_batches_tracked\", \"encoder.stages.2.1.conv3_1x1.conv.weight\", \"encoder.stages.2.1.conv3_1x1.bn.weight\", \"encoder.stages.2.1.conv3_1x1.bn.bias\", \"encoder.stages.2.1.conv3_1x1.bn.running_mean\", \"encoder.stages.2.1.conv3_1x1.bn.running_var\", \"encoder.stages.2.1.conv3_1x1.bn.num_batches_tracked\", \"encoder.stages.2.2.conv1_1x1.conv.weight\", \"encoder.stages.2.2.conv1_1x1.bn.weight\", \"encoder.stages.2.2.conv1_1x1.bn.bias\", \"encoder.stages.2.2.conv1_1x1.bn.running_mean\", \"encoder.stages.2.2.conv1_1x1.bn.running_var\", \"encoder.stages.2.2.conv1_1x1.bn.num_batches_tracked\", \"encoder.stages.2.2.conv2_kxk.conv.weight\", \"encoder.stages.2.2.conv2_kxk.bn.weight\", \"encoder.stages.2.2.conv2_kxk.bn.bias\", \"encoder.stages.2.2.conv2_kxk.bn.running_mean\", \"encoder.stages.2.2.conv2_kxk.bn.running_var\", \"encoder.stages.2.2.conv2_kxk.bn.num_batches_tracked\", \"encoder.stages.2.2.conv3_1x1.conv.weight\", \"encoder.stages.2.2.conv3_1x1.bn.weight\", \"encoder.stages.2.2.conv3_1x1.bn.bias\", \"encoder.stages.2.2.conv3_1x1.bn.running_mean\", \"encoder.stages.2.2.conv3_1x1.bn.running_var\", \"encoder.stages.2.2.conv3_1x1.bn.num_batches_tracked\", \"encoder.stages.2.3.conv1_1x1.conv.weight\", \"encoder.stages.2.3.conv1_1x1.bn.weight\", \"encoder.stages.2.3.conv1_1x1.bn.bias\", \"encoder.stages.2.3.conv1_1x1.bn.running_mean\", \"encoder.stages.2.3.conv1_1x1.bn.running_var\", \"encoder.stages.2.3.conv1_1x1.bn.num_batches_tracked\", \"encoder.stages.2.3.conv2_kxk.conv.weight\", \"encoder.stages.2.3.conv2_kxk.bn.weight\", \"encoder.stages.2.3.conv2_kxk.bn.bias\", \"encoder.stages.2.3.conv2_kxk.bn.running_mean\", \"encoder.stages.2.3.conv2_kxk.bn.running_var\", \"encoder.stages.2.3.conv2_kxk.bn.num_batches_tracked\", \"encoder.stages.2.3.conv3_1x1.conv.weight\", \"encoder.stages.2.3.conv3_1x1.bn.weight\", \"encoder.stages.2.3.conv3_1x1.bn.bias\", \"encoder.stages.2.3.conv3_1x1.bn.running_mean\", \"encoder.stages.2.3.conv3_1x1.bn.running_var\", \"encoder.stages.2.3.conv3_1x1.bn.num_batches_tracked\", \"encoder.stages.2.4.conv1_1x1.conv.weight\", \"encoder.stages.2.4.conv1_1x1.bn.weight\", \"encoder.stages.2.4.conv1_1x1.bn.bias\", \"encoder.stages.2.4.conv1_1x1.bn.running_mean\", \"encoder.stages.2.4.conv1_1x1.bn.running_var\", \"encoder.stages.2.4.conv1_1x1.bn.num_batches_tracked\", \"encoder.stages.2.4.conv2_kxk.conv.weight\", \"encoder.stages.2.4.conv2_kxk.bn.weight\", \"encoder.stages.2.4.conv2_kxk.bn.bias\", \"encoder.stages.2.4.conv2_kxk.bn.running_mean\", \"encoder.stages.2.4.conv2_kxk.bn.running_var\", \"encoder.stages.2.4.conv2_kxk.bn.num_batches_tracked\", \"encoder.stages.2.4.conv3_1x1.conv.weight\", \"encoder.stages.2.4.conv3_1x1.bn.weight\", \"encoder.stages.2.4.conv3_1x1.bn.bias\", \"encoder.stages.2.4.conv3_1x1.bn.running_mean\", \"encoder.stages.2.4.conv3_1x1.bn.running_var\", \"encoder.stages.2.4.conv3_1x1.bn.num_batches_tracked\", \"encoder.stages.2.5.conv1_1x1.conv.weight\", \"encoder.stages.2.5.conv1_1x1.bn.weight\", \"encoder.stages.2.5.conv1_1x1.bn.bias\", \"encoder.stages.2.5.conv1_1x1.bn.running_mean\", \"encoder.stages.2.5.conv1_1x1.bn.running_var\", \"encoder.stages.2.5.conv1_1x1.bn.num_batches_tracked\", \"encoder.stages.2.5.conv2_kxk.conv.weight\", \"encoder.stages.2.5.conv2_kxk.bn.weight\", \"encoder.stages.2.5.conv2_kxk.bn.bias\", \"encoder.stages.2.5.conv2_kxk.bn.running_mean\", \"encoder.stages.2.5.conv2_kxk.bn.running_var\", \"encoder.stages.2.5.conv2_kxk.bn.num_batches_tracked\", \"encoder.stages.2.5.conv3_1x1.conv.weight\", \"encoder.stages.2.5.conv3_1x1.bn.weight\", \"encoder.stages.2.5.conv3_1x1.bn.bias\", \"encoder.stages.2.5.conv3_1x1.bn.running_mean\", \"encoder.stages.2.5.conv3_1x1.bn.running_var\", \"encoder.stages.2.5.conv3_1x1.bn.num_batches_tracked\", \"encoder.stages.3.0.shortcut.conv.weight\", \"encoder.stages.3.0.shortcut.bn.weight\", \"encoder.stages.3.0.shortcut.bn.bias\", \"encoder.stages.3.0.shortcut.bn.running_mean\", \"encoder.stages.3.0.shortcut.bn.running_var\", \"encoder.stages.3.0.shortcut.bn.num_batches_tracked\", \"encoder.stages.3.0.conv1_1x1.conv.weight\", \"encoder.stages.3.0.conv1_1x1.bn.weight\", \"encoder.stages.3.0.conv1_1x1.bn.bias\", \"encoder.stages.3.0.conv1_1x1.bn.running_mean\", \"encoder.stages.3.0.conv1_1x1.bn.running_var\", \"encoder.stages.3.0.conv1_1x1.bn.num_batches_tracked\", \"encoder.stages.3.0.conv2_kxk.conv.weight\", \"encoder.stages.3.0.conv2_kxk.bn.weight\", \"encoder.stages.3.0.conv2_kxk.bn.bias\", \"encoder.stages.3.0.conv2_kxk.bn.running_mean\", \"encoder.stages.3.0.conv2_kxk.bn.running_var\", \"encoder.stages.3.0.conv2_kxk.bn.num_batches_tracked\", \"encoder.stages.3.0.conv3_1x1.conv.weight\", \"encoder.stages.3.0.conv3_1x1.bn.weight\", \"encoder.stages.3.0.conv3_1x1.bn.bias\", \"encoder.stages.3.0.conv3_1x1.bn.running_mean\", \"encoder.stages.3.0.conv3_1x1.bn.running_var\", \"encoder.stages.3.0.conv3_1x1.bn.num_batches_tracked\", \"encoder.stages.3.1.conv1_1x1.conv.weight\", \"encoder.stages.3.1.conv1_1x1.bn.weight\", \"encoder.stages.3.1.conv1_1x1.bn.bias\", \"encoder.stages.3.1.conv1_1x1.bn.running_mean\", \"encoder.stages.3.1.conv1_1x1.bn.running_var\", \"encoder.stages.3.1.conv1_1x1.bn.num_batches_tracked\", \"encoder.stages.3.1.conv2_kxk.conv.weight\", \"encoder.stages.3.1.conv2_kxk.bn.weight\", \"encoder.stages.3.1.conv2_kxk.bn.bias\", \"encoder.stages.3.1.conv2_kxk.bn.running_mean\", \"encoder.stages.3.1.conv2_kxk.bn.running_var\", \"encoder.stages.3.1.conv2_kxk.bn.num_batches_tracked\", \"encoder.stages.3.1.conv3_1x1.conv.weight\", \"encoder.stages.3.1.conv3_1x1.bn.weight\", \"encoder.stages.3.1.conv3_1x1.bn.bias\", \"encoder.stages.3.1.conv3_1x1.bn.running_mean\", \"encoder.stages.3.1.conv3_1x1.bn.running_var\", \"encoder.stages.3.1.conv3_1x1.bn.num_batches_tracked\", \"encoder.stages.3.2.conv1_1x1.conv.weight\", \"encoder.stages.3.2.conv1_1x1.bn.weight\", \"encoder.stages.3.2.conv1_1x1.bn.bias\", \"encoder.stages.3.2.conv1_1x1.bn.running_mean\", \"encoder.stages.3.2.conv1_1x1.bn.running_var\", \"encoder.stages.3.2.conv1_1x1.bn.num_batches_tracked\", \"encoder.stages.3.2.conv2_kxk.conv.weight\", \"encoder.stages.3.2.conv2_kxk.bn.weight\", \"encoder.stages.3.2.conv2_kxk.bn.bias\", \"encoder.stages.3.2.conv2_kxk.bn.running_mean\", \"encoder.stages.3.2.conv2_kxk.bn.running_var\", \"encoder.stages.3.2.conv2_kxk.bn.num_batches_tracked\", \"encoder.stages.3.2.conv3_1x1.conv.weight\", \"encoder.stages.3.2.conv3_1x1.bn.weight\", \"encoder.stages.3.2.conv3_1x1.bn.bias\", \"encoder.stages.3.2.conv3_1x1.bn.running_mean\", \"encoder.stages.3.2.conv3_1x1.bn.running_var\", \"encoder.stages.3.2.conv3_1x1.bn.num_batches_tracked\", \"encoder.stages.3.3.conv1_1x1.conv.weight\", \"encoder.stages.3.3.conv1_1x1.bn.weight\", \"encoder.stages.3.3.conv1_1x1.bn.bias\", \"encoder.stages.3.3.conv1_1x1.bn.running_mean\", \"encoder.stages.3.3.conv1_1x1.bn.running_var\", \"encoder.stages.3.3.conv1_1x1.bn.num_batches_tracked\", \"encoder.stages.3.3.conv2_kxk.conv.weight\", \"encoder.stages.3.3.conv2_kxk.bn.weight\", \"encoder.stages.3.3.conv2_kxk.bn.bias\", \"encoder.stages.3.3.conv2_kxk.bn.running_mean\", \"encoder.stages.3.3.conv2_kxk.bn.running_var\", \"encoder.stages.3.3.conv2_kxk.bn.num_batches_tracked\", \"encoder.stages.3.3.conv3_1x1.conv.weight\", \"encoder.stages.3.3.conv3_1x1.bn.weight\", \"encoder.stages.3.3.conv3_1x1.bn.bias\", \"encoder.stages.3.3.conv3_1x1.bn.running_mean\", \"encoder.stages.3.3.conv3_1x1.bn.running_var\", \"encoder.stages.3.3.conv3_1x1.bn.num_batches_tracked\", \"encoder.stages.3.4.conv1_1x1.conv.weight\", \"encoder.stages.3.4.conv1_1x1.bn.weight\", \"encoder.stages.3.4.conv1_1x1.bn.bias\", \"encoder.stages.3.4.conv1_1x1.bn.running_mean\", \"encoder.stages.3.4.conv1_1x1.bn.running_var\", \"encoder.stages.3.4.conv1_1x1.bn.num_batches_tracked\", \"encoder.stages.3.4.conv2_kxk.conv.weight\", \"encoder.stages.3.4.conv2_kxk.bn.weight\", \"encoder.stages.3.4.conv2_kxk.bn.bias\", \"encoder.stages.3.4.conv2_kxk.bn.running_mean\", \"encoder.stages.3.4.conv2_kxk.bn.running_var\", \"encoder.stages.3.4.conv2_kxk.bn.num_batches_tracked\", \"encoder.stages.3.4.conv3_1x1.conv.weight\", \"encoder.stages.3.4.conv3_1x1.bn.weight\", \"encoder.stages.3.4.conv3_1x1.bn.bias\", \"encoder.stages.3.4.conv3_1x1.bn.running_mean\", \"encoder.stages.3.4.conv3_1x1.bn.running_var\", \"encoder.stages.3.4.conv3_1x1.bn.num_batches_tracked\", \"encoder.stages.4.0.conv1_1x1.conv.weight\", \"encoder.stages.4.0.conv1_1x1.bn.weight\", \"encoder.stages.4.0.conv1_1x1.bn.bias\", \"encoder.stages.4.0.conv1_1x1.bn.running_mean\", \"encoder.stages.4.0.conv1_1x1.bn.running_var\", \"encoder.stages.4.0.conv1_1x1.bn.num_batches_tracked\", \"encoder.stages.4.0.conv2_kxk.conv.weight\", \"encoder.stages.4.0.conv2_kxk.bn.weight\", \"encoder.stages.4.0.conv2_kxk.bn.bias\", \"encoder.stages.4.0.conv2_kxk.bn.running_mean\", \"encoder.stages.4.0.conv2_kxk.bn.running_var\", \"encoder.stages.4.0.conv2_kxk.bn.num_batches_tracked\", \"encoder.stages.4.0.conv3_1x1.conv.weight\", \"encoder.stages.4.0.conv3_1x1.bn.weight\", \"encoder.stages.4.0.conv3_1x1.bn.bias\", \"encoder.stages.4.0.conv3_1x1.bn.running_mean\", \"encoder.stages.4.0.conv3_1x1.bn.running_var\", \"encoder.stages.4.0.conv3_1x1.bn.num_batches_tracked\", \"encoder.stages.4.1.conv1_1x1.conv.weight\", \"encoder.stages.4.1.conv1_1x1.bn.weight\", \"encoder.stages.4.1.conv1_1x1.bn.bias\", \"encoder.stages.4.1.conv1_1x1.bn.running_mean\", \"encoder.stages.4.1.conv1_1x1.bn.running_var\", \"encoder.stages.4.1.conv1_1x1.bn.num_batches_tracked\", \"encoder.stages.4.1.conv2_kxk.conv.weight\", \"encoder.stages.4.1.conv2_kxk.bn.weight\", \"encoder.stages.4.1.conv2_kxk.bn.bias\", \"encoder.stages.4.1.conv2_kxk.bn.running_mean\", \"encoder.stages.4.1.conv2_kxk.bn.running_var\", \"encoder.stages.4.1.conv2_kxk.bn.num_batches_tracked\", \"encoder.stages.4.1.conv3_1x1.conv.weight\", \"encoder.stages.4.1.conv3_1x1.bn.weight\", \"encoder.stages.4.1.conv3_1x1.bn.bias\", \"encoder.stages.4.1.conv3_1x1.bn.running_mean\", \"encoder.stages.4.1.conv3_1x1.bn.running_var\", \"encoder.stages.4.1.conv3_1x1.bn.num_batches_tracked\", \"encoder.stages.4.2.conv1_1x1.conv.weight\", \"encoder.stages.4.2.conv1_1x1.bn.weight\", \"encoder.stages.4.2.conv1_1x1.bn.bias\", \"encoder.stages.4.2.conv1_1x1.bn.running_mean\", \"encoder.stages.4.2.conv1_1x1.bn.running_var\", \"encoder.stages.4.2.conv1_1x1.bn.num_batches_tracked\", \"encoder.stages.4.2.conv2_kxk.conv.weight\", \"encoder.stages.4.2.conv2_kxk.bn.weight\", \"encoder.stages.4.2.conv2_kxk.bn.bias\", \"encoder.stages.4.2.conv2_kxk.bn.running_mean\", \"encoder.stages.4.2.conv2_kxk.bn.running_var\", \"encoder.stages.4.2.conv2_kxk.bn.num_batches_tracked\", \"encoder.stages.4.2.conv3_1x1.conv.weight\", \"encoder.stages.4.2.conv3_1x1.bn.weight\", \"encoder.stages.4.2.conv3_1x1.bn.bias\", \"encoder.stages.4.2.conv3_1x1.bn.running_mean\", \"encoder.stages.4.2.conv3_1x1.bn.running_var\", \"encoder.stages.4.2.conv3_1x1.bn.num_batches_tracked\", \"encoder.stages.4.3.conv1_1x1.conv.weight\", \"encoder.stages.4.3.conv1_1x1.bn.weight\", \"encoder.stages.4.3.conv1_1x1.bn.bias\", \"encoder.stages.4.3.conv1_1x1.bn.running_mean\", \"encoder.stages.4.3.conv1_1x1.bn.running_var\", \"encoder.stages.4.3.conv1_1x1.bn.num_batches_tracked\", \"encoder.stages.4.3.conv2_kxk.conv.weight\", \"encoder.stages.4.3.conv2_kxk.bn.weight\", \"encoder.stages.4.3.conv2_kxk.bn.bias\", \"encoder.stages.4.3.conv2_kxk.bn.running_mean\", \"encoder.stages.4.3.conv2_kxk.bn.running_var\", \"encoder.stages.4.3.conv2_kxk.bn.num_batches_tracked\", \"encoder.stages.4.3.conv3_1x1.conv.weight\", \"encoder.stages.4.3.conv3_1x1.bn.weight\", \"encoder.stages.4.3.conv3_1x1.bn.bias\", \"encoder.stages.4.3.conv3_1x1.bn.running_mean\", \"encoder.stages.4.3.conv3_1x1.bn.running_var\", \"encoder.stages.4.3.conv3_1x1.bn.num_batches_tracked\", \"encoder.final_conv.conv.weight\", \"encoder.final_conv.bn.weight\", \"encoder.final_conv.bn.bias\", \"encoder.final_conv.bn.running_mean\", \"encoder.final_conv.bn.running_var\", \"encoder.final_conv.bn.num_batches_tracked\". \n\tsize mismatch for decoder.blocks.0.conv1.0.weight: copying a param with shape torch.Size([256, 3200, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 1072, 3, 3]).\n\tsize mismatch for decoder.blocks.1.conv1.0.weight: copying a param with shape torch.Size([128, 448, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 296, 3, 3]).\n\tsize mismatch for decoder.blocks.2.conv1.0.weight: copying a param with shape torch.Size([64, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 152, 3, 3]).\n\tsize mismatch for decoder.blocks.3.conv1.0.weight: copying a param with shape torch.Size([32, 96, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 80, 3, 3])."],"ename":"RuntimeError","evalue":"Error(s) in loading state_dict for Unet:\n\tMissing key(s) in state_dict: \"encoder.model.conv_stem.weight\", \"encoder.model.bn1.weight\", \"encoder.model.bn1.bias\", \"encoder.model.bn1.running_mean\", \"encoder.model.bn1.running_var\", \"encoder.model.blocks.0.0.conv_dw.weight\", \"encoder.model.blocks.0.0.bn1.weight\", \"encoder.model.blocks.0.0.bn1.bias\", \"encoder.model.blocks.0.0.bn1.running_mean\", \"encoder.model.blocks.0.0.bn1.running_var\", \"encoder.model.blocks.0.0.conv_pw.weight\", \"encoder.model.blocks.0.0.bn2.weight\", \"encoder.model.blocks.0.0.bn2.bias\", \"encoder.model.blocks.0.0.bn2.running_mean\", \"encoder.model.blocks.0.0.bn2.running_var\", \"encoder.model.blocks.1.0.conv_pw.weight\", \"encoder.model.blocks.1.0.bn1.weight\", \"encoder.model.blocks.1.0.bn1.bias\", \"encoder.model.blocks.1.0.bn1.running_mean\", \"encoder.model.blocks.1.0.bn1.running_var\", \"encoder.model.blocks.1.0.conv_dw.weight\", \"encoder.model.blocks.1.0.bn2.weight\", \"encoder.model.blocks.1.0.bn2.bias\", \"encoder.model.blocks.1.0.bn2.running_mean\", \"encoder.model.blocks.1.0.bn2.running_var\", \"encoder.model.blocks.1.0.conv_pwl.weight\", \"encoder.model.blocks.1.0.bn3.weight\", \"encoder.model.blocks.1.0.bn3.bias\", \"encoder.model.blocks.1.0.bn3.running_mean\", \"encoder.model.blocks.1.0.bn3.running_var\", \"encoder.model.blocks.1.1.conv_pw.weight\", \"encoder.model.blocks.1.1.bn1.weight\", \"encoder.model.blocks.1.1.bn1.bias\", \"encoder.model.blocks.1.1.bn1.running_mean\", \"encoder.model.blocks.1.1.bn1.running_var\", \"encoder.model.blocks.1.1.conv_dw.weight\", \"encoder.model.blocks.1.1.bn2.weight\", \"encoder.model.blocks.1.1.bn2.bias\", \"encoder.model.blocks.1.1.bn2.running_mean\", \"encoder.model.blocks.1.1.bn2.running_var\", \"encoder.model.blocks.1.1.conv_pwl.weight\", \"encoder.model.blocks.1.1.bn3.weight\", \"encoder.model.blocks.1.1.bn3.bias\", \"encoder.model.blocks.1.1.bn3.running_mean\", \"encoder.model.blocks.1.1.bn3.running_var\", \"encoder.model.blocks.2.0.conv_pw.weight\", \"encoder.model.blocks.2.0.bn1.weight\", \"encoder.model.blocks.2.0.bn1.bias\", \"encoder.model.blocks.2.0.bn1.running_mean\", \"encoder.model.blocks.2.0.bn1.running_var\", \"encoder.model.blocks.2.0.conv_dw.weight\", \"encoder.model.blocks.2.0.bn2.weight\", \"encoder.model.blocks.2.0.bn2.bias\", \"encoder.model.blocks.2.0.bn2.running_mean\", \"encoder.model.blocks.2.0.bn2.running_var\", \"encoder.model.blocks.2.0.se.conv_reduce.weight\", \"encoder.model.blocks.2.0.se.conv_reduce.bias\", \"encoder.model.blocks.2.0.se.conv_expand.weight\", \"encoder.model.blocks.2.0.se.conv_expand.bias\", \"encoder.model.blocks.2.0.conv_pwl.weight\", \"encoder.model.blocks.2.0.bn3.weight\", \"encoder.model.blocks.2.0.bn3.bias\", \"encoder.model.blocks.2.0.bn3.running_mean\", \"encoder.model.blocks.2.0.bn3.running_var\", \"encoder.model.blocks.2.1.conv_pw.weight\", \"encoder.model.blocks.2.1.bn1.weight\", \"encoder.model.blocks.2.1.bn1.bias\", \"encoder.model.blocks.2.1.bn1.running_mean\", \"encoder.model.blocks.2.1.bn1.running_var\", \"encoder.model.blocks.2.1.conv_dw.weight\", \"encoder.model.blocks.2.1.bn2.weight\", \"encoder.model.blocks.2.1.bn2.bias\", \"encoder.model.blocks.2.1.bn2.running_mean\", \"encoder.model.blocks.2.1.bn2.running_var\", \"encoder.model.blocks.2.1.se.conv_reduce.weight\", \"encoder.model.blocks.2.1.se.conv_reduce.bias\", \"encoder.model.blocks.2.1.se.conv_expand.weight\", \"encoder.model.blocks.2.1.se.conv_expand.bias\", \"encoder.model.blocks.2.1.conv_pwl.weight\", \"encoder.model.blocks.2.1.bn3.weight\", \"encoder.model.blocks.2.1.bn3.bias\", \"encoder.model.blocks.2.1.bn3.running_mean\", \"encoder.model.blocks.2.1.bn3.running_var\", \"encoder.model.blocks.2.2.conv_pw.weight\", \"encoder.model.blocks.2.2.bn1.weight\", \"encoder.model.blocks.2.2.bn1.bias\", \"encoder.model.blocks.2.2.bn1.running_mean\", \"encoder.model.blocks.2.2.bn1.running_var\", \"encoder.model.blocks.2.2.conv_dw.weight\", \"encoder.model.blocks.2.2.bn2.weight\", \"encoder.model.blocks.2.2.bn2.bias\", \"encoder.model.blocks.2.2.bn2.running_mean\", \"encoder.model.blocks.2.2.bn2.running_var\", \"encoder.model.blocks.2.2.se.conv_reduce.weight\", \"encoder.model.blocks.2.2.se.conv_reduce.bias\", \"encoder.model.blocks.2.2.se.conv_expand.weight\", \"encoder.model.blocks.2.2.se.conv_expand.bias\", \"encoder.model.blocks.2.2.conv_pwl.weight\", \"encoder.model.blocks.2.2.bn3.weight\", \"encoder.model.blocks.2.2.bn3.bias\", \"encoder.model.blocks.2.2.bn3.running_mean\", \"encoder.model.blocks.2.2.bn3.running_var\", \"encoder.model.blocks.3.0.conv_pw.weight\", \"encoder.model.blocks.3.0.bn1.weight\", \"encoder.model.blocks.3.0.bn1.bias\", \"encoder.model.blocks.3.0.bn1.running_mean\", \"encoder.model.blocks.3.0.bn1.running_var\", \"encoder.model.blocks.3.0.conv_dw.weight\", \"encoder.model.blocks.3.0.bn2.weight\", \"encoder.model.blocks.3.0.bn2.bias\", \"encoder.model.blocks.3.0.bn2.running_mean\", \"encoder.model.blocks.3.0.bn2.running_var\", \"encoder.model.blocks.3.0.conv_pwl.weight\", \"encoder.model.blocks.3.0.bn3.weight\", \"encoder.model.blocks.3.0.bn3.bias\", \"encoder.model.blocks.3.0.bn3.running_mean\", \"encoder.model.blocks.3.0.bn3.running_var\", \"encoder.model.blocks.3.1.conv_pw.weight\", \"encoder.model.blocks.3.1.bn1.weight\", \"encoder.model.blocks.3.1.bn1.bias\", \"encoder.model.blocks.3.1.bn1.running_mean\", \"encoder.model.blocks.3.1.bn1.running_var\", \"encoder.model.blocks.3.1.conv_dw.weight\", \"encoder.model.blocks.3.1.bn2.weight\", \"encoder.model.blocks.3.1.bn2.bias\", \"encoder.model.blocks.3.1.bn2.running_mean\", \"encoder.model.blocks.3.1.bn2.running_var\", \"encoder.model.blocks.3.1.conv_pwl.weight\", \"encoder.model.blocks.3.1.bn3.weight\", \"encoder.model.blocks.3.1.bn3.bias\", \"encoder.model.blocks.3.1.bn3.running_mean\", \"encoder.model.blocks.3.1.bn3.running_var\", \"encoder.model.blocks.3.2.conv_pw.weight\", \"encoder.model.blocks.3.2.bn1.weight\", \"encoder.model.blocks.3.2.bn1.bias\", \"encoder.model.blocks.3.2.bn1.running_mean\", \"encoder.model.blocks.3.2.bn1.running_var\", \"encoder.model.blocks.3.2.conv_dw.weight\", \"encoder.model.blocks.3.2.bn2.weight\", \"encoder.model.blocks.3.2.bn2.bias\", \"encoder.model.blocks.3.2.bn2.running_mean\", \"encoder.model.blocks.3.2.bn2.running_var\", \"encoder.model.blocks.3.2.conv_pwl.weight\", \"encoder.model.blocks.3.2.bn3.weight\", \"encoder.model.blocks.3.2.bn3.bias\", \"encoder.model.blocks.3.2.bn3.running_mean\", \"encoder.model.blocks.3.2.bn3.running_var\", \"encoder.model.blocks.3.3.conv_pw.weight\", \"encoder.model.blocks.3.3.bn1.weight\", \"encoder.model.blocks.3.3.bn1.bias\", \"encoder.model.blocks.3.3.bn1.running_mean\", \"encoder.model.blocks.3.3.bn1.running_var\", \"encoder.model.blocks.3.3.conv_dw.weight\", \"encoder.model.blocks.3.3.bn2.weight\", \"encoder.model.blocks.3.3.bn2.bias\", \"encoder.model.blocks.3.3.bn2.running_mean\", \"encoder.model.blocks.3.3.bn2.running_var\", \"encoder.model.blocks.3.3.conv_pwl.weight\", \"encoder.model.blocks.3.3.bn3.weight\", \"encoder.model.blocks.3.3.bn3.bias\", \"encoder.model.blocks.3.3.bn3.running_mean\", \"encoder.model.blocks.3.3.bn3.running_var\", \"encoder.model.blocks.4.0.conv_pw.weight\", \"encoder.model.blocks.4.0.bn1.weight\", \"encoder.model.blocks.4.0.bn1.bias\", \"encoder.model.blocks.4.0.bn1.running_mean\", \"encoder.model.blocks.4.0.bn1.running_var\", \"encoder.model.blocks.4.0.conv_dw.weight\", \"encoder.model.blocks.4.0.bn2.weight\", \"encoder.model.blocks.4.0.bn2.bias\", \"encoder.model.blocks.4.0.bn2.running_mean\", \"encoder.model.blocks.4.0.bn2.running_var\", \"encoder.model.blocks.4.0.se.conv_reduce.weight\", \"encoder.model.blocks.4.0.se.conv_reduce.bias\", \"encoder.model.blocks.4.0.se.conv_expand.weight\", \"encoder.model.blocks.4.0.se.conv_expand.bias\", \"encoder.model.blocks.4.0.conv_pwl.weight\", \"encoder.model.blocks.4.0.bn3.weight\", \"encoder.model.blocks.4.0.bn3.bias\", \"encoder.model.blocks.4.0.bn3.running_mean\", \"encoder.model.blocks.4.0.bn3.running_var\", \"encoder.model.blocks.4.1.conv_pw.weight\", \"encoder.model.blocks.4.1.bn1.weight\", \"encoder.model.blocks.4.1.bn1.bias\", \"encoder.model.blocks.4.1.bn1.running_mean\", \"encoder.model.blocks.4.1.bn1.running_var\", \"encoder.model.blocks.4.1.conv_dw.weight\", \"encoder.model.blocks.4.1.bn2.weight\", \"encoder.model.blocks.4.1.bn2.bias\", \"encoder.model.blocks.4.1.bn2.running_mean\", \"encoder.model.blocks.4.1.bn2.running_var\", \"encoder.model.blocks.4.1.se.conv_reduce.weight\", \"encoder.model.blocks.4.1.se.conv_reduce.bias\", \"encoder.model.blocks.4.1.se.conv_expand.weight\", \"encoder.model.blocks.4.1.se.conv_expand.bias\", \"encoder.model.blocks.4.1.conv_pwl.weight\", \"encoder.model.blocks.4.1.bn3.weight\", \"encoder.model.blocks.4.1.bn3.bias\", \"encoder.model.blocks.4.1.bn3.running_mean\", \"encoder.model.blocks.4.1.bn3.running_var\", \"encoder.model.blocks.5.0.conv_pw.weight\", \"encoder.model.blocks.5.0.bn1.weight\", \"encoder.model.blocks.5.0.bn1.bias\", \"encoder.model.blocks.5.0.bn1.running_mean\", \"encoder.model.blocks.5.0.bn1.running_var\", \"encoder.model.blocks.5.0.conv_dw.weight\", \"encoder.model.blocks.5.0.bn2.weight\", \"encoder.model.blocks.5.0.bn2.bias\", \"encoder.model.blocks.5.0.bn2.running_mean\", \"encoder.model.blocks.5.0.bn2.running_var\", \"encoder.model.blocks.5.0.se.conv_reduce.weight\", \"encoder.model.blocks.5.0.se.conv_reduce.bias\", \"encoder.model.blocks.5.0.se.conv_expand.weight\", \"encoder.model.blocks.5.0.se.conv_expand.bias\", \"encoder.model.blocks.5.0.conv_pwl.weight\", \"encoder.model.blocks.5.0.bn3.weight\", \"encoder.model.blocks.5.0.bn3.bias\", \"encoder.model.blocks.5.0.bn3.running_mean\", \"encoder.model.blocks.5.0.bn3.running_var\", \"encoder.model.blocks.5.1.conv_pw.weight\", \"encoder.model.blocks.5.1.bn1.weight\", \"encoder.model.blocks.5.1.bn1.bias\", \"encoder.model.blocks.5.1.bn1.running_mean\", \"encoder.model.blocks.5.1.bn1.running_var\", \"encoder.model.blocks.5.1.conv_dw.weight\", \"encoder.model.blocks.5.1.bn2.weight\", \"encoder.model.blocks.5.1.bn2.bias\", \"encoder.model.blocks.5.1.bn2.running_mean\", \"encoder.model.blocks.5.1.bn2.running_var\", \"encoder.model.blocks.5.1.se.conv_reduce.weight\", \"encoder.model.blocks.5.1.se.conv_reduce.bias\", \"encoder.model.blocks.5.1.se.conv_expand.weight\", \"encoder.model.blocks.5.1.se.conv_expand.bias\", \"encoder.model.blocks.5.1.conv_pwl.weight\", \"encoder.model.blocks.5.1.bn3.weight\", \"encoder.model.blocks.5.1.bn3.bias\", \"encoder.model.blocks.5.1.bn3.running_mean\", \"encoder.model.blocks.5.1.bn3.running_var\", \"encoder.model.blocks.5.2.conv_pw.weight\", \"encoder.model.blocks.5.2.bn1.weight\", \"encoder.model.blocks.5.2.bn1.bias\", \"encoder.model.blocks.5.2.bn1.running_mean\", \"encoder.model.blocks.5.2.bn1.running_var\", \"encoder.model.blocks.5.2.conv_dw.weight\", \"encoder.model.blocks.5.2.bn2.weight\", \"encoder.model.blocks.5.2.bn2.bias\", \"encoder.model.blocks.5.2.bn2.running_mean\", \"encoder.model.blocks.5.2.bn2.running_var\", \"encoder.model.blocks.5.2.se.conv_reduce.weight\", \"encoder.model.blocks.5.2.se.conv_reduce.bias\", \"encoder.model.blocks.5.2.se.conv_expand.weight\", \"encoder.model.blocks.5.2.se.conv_expand.bias\", \"encoder.model.blocks.5.2.conv_pwl.weight\", \"encoder.model.blocks.5.2.bn3.weight\", \"encoder.model.blocks.5.2.bn3.bias\", \"encoder.model.blocks.5.2.bn3.running_mean\", \"encoder.model.blocks.5.2.bn3.running_var\", \"encoder.model.blocks.6.0.conv.weight\", \"encoder.model.blocks.6.0.bn1.weight\", \"encoder.model.blocks.6.0.bn1.bias\", \"encoder.model.blocks.6.0.bn1.running_mean\", \"encoder.model.blocks.6.0.bn1.running_var\". \n\tUnexpected key(s) in state_dict: \"encoder.stem.conv.weight\", \"encoder.stem.bn.weight\", \"encoder.stem.bn.bias\", \"encoder.stem.bn.running_mean\", \"encoder.stem.bn.running_var\", \"encoder.stem.bn.num_batches_tracked\", \"encoder.stages.0.0.shortcut.conv.weight\", \"encoder.stages.0.0.shortcut.bn.weight\", \"encoder.stages.0.0.shortcut.bn.bias\", \"encoder.stages.0.0.shortcut.bn.running_mean\", \"encoder.stages.0.0.shortcut.bn.running_var\", \"encoder.stages.0.0.shortcut.bn.num_batches_tracked\", \"encoder.stages.0.0.conv1_kxk.conv.weight\", \"encoder.stages.0.0.conv1_kxk.bn.weight\", \"encoder.stages.0.0.conv1_kxk.bn.bias\", \"encoder.stages.0.0.conv1_kxk.bn.running_mean\", \"encoder.stages.0.0.conv1_kxk.bn.running_var\", \"encoder.stages.0.0.conv1_kxk.bn.num_batches_tracked\", \"encoder.stages.0.0.conv2_kxk.conv.weight\", \"encoder.stages.0.0.conv2_kxk.bn.weight\", \"encoder.stages.0.0.conv2_kxk.bn.bias\", \"encoder.stages.0.0.conv2_kxk.bn.running_mean\", \"encoder.stages.0.0.conv2_kxk.bn.running_var\", \"encoder.stages.0.0.conv2_kxk.bn.num_batches_tracked\", \"encoder.stages.1.0.shortcut.conv.weight\", \"encoder.stages.1.0.shortcut.bn.weight\", \"encoder.stages.1.0.shortcut.bn.bias\", \"encoder.stages.1.0.shortcut.bn.running_mean\", \"encoder.stages.1.0.shortcut.bn.running_var\", \"encoder.stages.1.0.shortcut.bn.num_batches_tracked\", \"encoder.stages.1.0.conv1_kxk.conv.weight\", \"encoder.stages.1.0.conv1_kxk.bn.weight\", \"encoder.stages.1.0.conv1_kxk.bn.bias\", \"encoder.stages.1.0.conv1_kxk.bn.running_mean\", \"encoder.stages.1.0.conv1_kxk.bn.running_var\", \"encoder.stages.1.0.conv1_kxk.bn.num_batches_tracked\", \"encoder.stages.1.0.conv2_kxk.conv.weight\", \"encoder.stages.1.0.conv2_kxk.bn.weight\", \"encoder.stages.1.0.conv2_kxk.bn.bias\", \"encoder.stages.1.0.conv2_kxk.bn.running_mean\", \"encoder.stages.1.0.conv2_kxk.bn.running_var\", \"encoder.stages.1.0.conv2_kxk.bn.num_batches_tracked\", \"encoder.stages.1.1.conv1_kxk.conv.weight\", \"encoder.stages.1.1.conv1_kxk.bn.weight\", \"encoder.stages.1.1.conv1_kxk.bn.bias\", \"encoder.stages.1.1.conv1_kxk.bn.running_mean\", \"encoder.stages.1.1.conv1_kxk.bn.running_var\", \"encoder.stages.1.1.conv1_kxk.bn.num_batches_tracked\", \"encoder.stages.1.1.conv2_kxk.conv.weight\", \"encoder.stages.1.1.conv2_kxk.bn.weight\", \"encoder.stages.1.1.conv2_kxk.bn.bias\", \"encoder.stages.1.1.conv2_kxk.bn.running_mean\", \"encoder.stages.1.1.conv2_kxk.bn.running_var\", \"encoder.stages.1.1.conv2_kxk.bn.num_batches_tracked\", \"encoder.stages.2.0.shortcut.conv.weight\", \"encoder.stages.2.0.shortcut.bn.weight\", \"encoder.stages.2.0.shortcut.bn.bias\", \"encoder.stages.2.0.shortcut.bn.running_mean\", \"encoder.stages.2.0.shortcut.bn.running_var\", \"encoder.stages.2.0.shortcut.bn.num_batches_tracked\", \"encoder.stages.2.0.conv1_1x1.conv.weight\", \"encoder.stages.2.0.conv1_1x1.bn.weight\", \"encoder.stages.2.0.conv1_1x1.bn.bias\", \"encoder.stages.2.0.conv1_1x1.bn.running_mean\", \"encoder.stages.2.0.conv1_1x1.bn.running_var\", \"encoder.stages.2.0.conv1_1x1.bn.num_batches_tracked\", \"encoder.stages.2.0.conv2_kxk.conv.weight\", \"encoder.stages.2.0.conv2_kxk.bn.weight\", \"encoder.stages.2.0.conv2_kxk.bn.bias\", \"encoder.stages.2.0.conv2_kxk.bn.running_mean\", \"encoder.stages.2.0.conv2_kxk.bn.running_var\", \"encoder.stages.2.0.conv2_kxk.bn.num_batches_tracked\", \"encoder.stages.2.0.conv3_1x1.conv.weight\", \"encoder.stages.2.0.conv3_1x1.bn.weight\", \"encoder.stages.2.0.conv3_1x1.bn.bias\", \"encoder.stages.2.0.conv3_1x1.bn.running_mean\", \"encoder.stages.2.0.conv3_1x1.bn.running_var\", \"encoder.stages.2.0.conv3_1x1.bn.num_batches_tracked\", \"encoder.stages.2.1.conv1_1x1.conv.weight\", \"encoder.stages.2.1.conv1_1x1.bn.weight\", \"encoder.stages.2.1.conv1_1x1.bn.bias\", \"encoder.stages.2.1.conv1_1x1.bn.running_mean\", \"encoder.stages.2.1.conv1_1x1.bn.running_var\", \"encoder.stages.2.1.conv1_1x1.bn.num_batches_tracked\", \"encoder.stages.2.1.conv2_kxk.conv.weight\", \"encoder.stages.2.1.conv2_kxk.bn.weight\", \"encoder.stages.2.1.conv2_kxk.bn.bias\", \"encoder.stages.2.1.conv2_kxk.bn.running_mean\", \"encoder.stages.2.1.conv2_kxk.bn.running_var\", \"encoder.stages.2.1.conv2_kxk.bn.num_batches_tracked\", \"encoder.stages.2.1.conv3_1x1.conv.weight\", \"encoder.stages.2.1.conv3_1x1.bn.weight\", \"encoder.stages.2.1.conv3_1x1.bn.bias\", \"encoder.stages.2.1.conv3_1x1.bn.running_mean\", \"encoder.stages.2.1.conv3_1x1.bn.running_var\", \"encoder.stages.2.1.conv3_1x1.bn.num_batches_tracked\", \"encoder.stages.2.2.conv1_1x1.conv.weight\", \"encoder.stages.2.2.conv1_1x1.bn.weight\", \"encoder.stages.2.2.conv1_1x1.bn.bias\", \"encoder.stages.2.2.conv1_1x1.bn.running_mean\", \"encoder.stages.2.2.conv1_1x1.bn.running_var\", \"encoder.stages.2.2.conv1_1x1.bn.num_batches_tracked\", \"encoder.stages.2.2.conv2_kxk.conv.weight\", \"encoder.stages.2.2.conv2_kxk.bn.weight\", \"encoder.stages.2.2.conv2_kxk.bn.bias\", \"encoder.stages.2.2.conv2_kxk.bn.running_mean\", \"encoder.stages.2.2.conv2_kxk.bn.running_var\", \"encoder.stages.2.2.conv2_kxk.bn.num_batches_tracked\", \"encoder.stages.2.2.conv3_1x1.conv.weight\", \"encoder.stages.2.2.conv3_1x1.bn.weight\", \"encoder.stages.2.2.conv3_1x1.bn.bias\", \"encoder.stages.2.2.conv3_1x1.bn.running_mean\", \"encoder.stages.2.2.conv3_1x1.bn.running_var\", \"encoder.stages.2.2.conv3_1x1.bn.num_batches_tracked\", \"encoder.stages.2.3.conv1_1x1.conv.weight\", \"encoder.stages.2.3.conv1_1x1.bn.weight\", \"encoder.stages.2.3.conv1_1x1.bn.bias\", \"encoder.stages.2.3.conv1_1x1.bn.running_mean\", \"encoder.stages.2.3.conv1_1x1.bn.running_var\", \"encoder.stages.2.3.conv1_1x1.bn.num_batches_tracked\", \"encoder.stages.2.3.conv2_kxk.conv.weight\", \"encoder.stages.2.3.conv2_kxk.bn.weight\", \"encoder.stages.2.3.conv2_kxk.bn.bias\", \"encoder.stages.2.3.conv2_kxk.bn.running_mean\", \"encoder.stages.2.3.conv2_kxk.bn.running_var\", \"encoder.stages.2.3.conv2_kxk.bn.num_batches_tracked\", \"encoder.stages.2.3.conv3_1x1.conv.weight\", \"encoder.stages.2.3.conv3_1x1.bn.weight\", \"encoder.stages.2.3.conv3_1x1.bn.bias\", \"encoder.stages.2.3.conv3_1x1.bn.running_mean\", \"encoder.stages.2.3.conv3_1x1.bn.running_var\", \"encoder.stages.2.3.conv3_1x1.bn.num_batches_tracked\", \"encoder.stages.2.4.conv1_1x1.conv.weight\", \"encoder.stages.2.4.conv1_1x1.bn.weight\", \"encoder.stages.2.4.conv1_1x1.bn.bias\", \"encoder.stages.2.4.conv1_1x1.bn.running_mean\", \"encoder.stages.2.4.conv1_1x1.bn.running_var\", \"encoder.stages.2.4.conv1_1x1.bn.num_batches_tracked\", \"encoder.stages.2.4.conv2_kxk.conv.weight\", \"encoder.stages.2.4.conv2_kxk.bn.weight\", \"encoder.stages.2.4.conv2_kxk.bn.bias\", \"encoder.stages.2.4.conv2_kxk.bn.running_mean\", \"encoder.stages.2.4.conv2_kxk.bn.running_var\", \"encoder.stages.2.4.conv2_kxk.bn.num_batches_tracked\", \"encoder.stages.2.4.conv3_1x1.conv.weight\", \"encoder.stages.2.4.conv3_1x1.bn.weight\", \"encoder.stages.2.4.conv3_1x1.bn.bias\", \"encoder.stages.2.4.conv3_1x1.bn.running_mean\", \"encoder.stages.2.4.conv3_1x1.bn.running_var\", \"encoder.stages.2.4.conv3_1x1.bn.num_batches_tracked\", \"encoder.stages.2.5.conv1_1x1.conv.weight\", \"encoder.stages.2.5.conv1_1x1.bn.weight\", \"encoder.stages.2.5.conv1_1x1.bn.bias\", \"encoder.stages.2.5.conv1_1x1.bn.running_mean\", \"encoder.stages.2.5.conv1_1x1.bn.running_var\", \"encoder.stages.2.5.conv1_1x1.bn.num_batches_tracked\", \"encoder.stages.2.5.conv2_kxk.conv.weight\", \"encoder.stages.2.5.conv2_kxk.bn.weight\", \"encoder.stages.2.5.conv2_kxk.bn.bias\", \"encoder.stages.2.5.conv2_kxk.bn.running_mean\", \"encoder.stages.2.5.conv2_kxk.bn.running_var\", \"encoder.stages.2.5.conv2_kxk.bn.num_batches_tracked\", \"encoder.stages.2.5.conv3_1x1.conv.weight\", \"encoder.stages.2.5.conv3_1x1.bn.weight\", \"encoder.stages.2.5.conv3_1x1.bn.bias\", \"encoder.stages.2.5.conv3_1x1.bn.running_mean\", \"encoder.stages.2.5.conv3_1x1.bn.running_var\", \"encoder.stages.2.5.conv3_1x1.bn.num_batches_tracked\", \"encoder.stages.3.0.shortcut.conv.weight\", \"encoder.stages.3.0.shortcut.bn.weight\", \"encoder.stages.3.0.shortcut.bn.bias\", \"encoder.stages.3.0.shortcut.bn.running_mean\", \"encoder.stages.3.0.shortcut.bn.running_var\", \"encoder.stages.3.0.shortcut.bn.num_batches_tracked\", \"encoder.stages.3.0.conv1_1x1.conv.weight\", \"encoder.stages.3.0.conv1_1x1.bn.weight\", \"encoder.stages.3.0.conv1_1x1.bn.bias\", \"encoder.stages.3.0.conv1_1x1.bn.running_mean\", \"encoder.stages.3.0.conv1_1x1.bn.running_var\", \"encoder.stages.3.0.conv1_1x1.bn.num_batches_tracked\", \"encoder.stages.3.0.conv2_kxk.conv.weight\", \"encoder.stages.3.0.conv2_kxk.bn.weight\", \"encoder.stages.3.0.conv2_kxk.bn.bias\", \"encoder.stages.3.0.conv2_kxk.bn.running_mean\", \"encoder.stages.3.0.conv2_kxk.bn.running_var\", \"encoder.stages.3.0.conv2_kxk.bn.num_batches_tracked\", \"encoder.stages.3.0.conv3_1x1.conv.weight\", \"encoder.stages.3.0.conv3_1x1.bn.weight\", \"encoder.stages.3.0.conv3_1x1.bn.bias\", \"encoder.stages.3.0.conv3_1x1.bn.running_mean\", \"encoder.stages.3.0.conv3_1x1.bn.running_var\", \"encoder.stages.3.0.conv3_1x1.bn.num_batches_tracked\", \"encoder.stages.3.1.conv1_1x1.conv.weight\", \"encoder.stages.3.1.conv1_1x1.bn.weight\", \"encoder.stages.3.1.conv1_1x1.bn.bias\", \"encoder.stages.3.1.conv1_1x1.bn.running_mean\", \"encoder.stages.3.1.conv1_1x1.bn.running_var\", \"encoder.stages.3.1.conv1_1x1.bn.num_batches_tracked\", \"encoder.stages.3.1.conv2_kxk.conv.weight\", \"encoder.stages.3.1.conv2_kxk.bn.weight\", \"encoder.stages.3.1.conv2_kxk.bn.bias\", \"encoder.stages.3.1.conv2_kxk.bn.running_mean\", \"encoder.stages.3.1.conv2_kxk.bn.running_var\", \"encoder.stages.3.1.conv2_kxk.bn.num_batches_tracked\", \"encoder.stages.3.1.conv3_1x1.conv.weight\", \"encoder.stages.3.1.conv3_1x1.bn.weight\", \"encoder.stages.3.1.conv3_1x1.bn.bias\", \"encoder.stages.3.1.conv3_1x1.bn.running_mean\", \"encoder.stages.3.1.conv3_1x1.bn.running_var\", \"encoder.stages.3.1.conv3_1x1.bn.num_batches_tracked\", \"encoder.stages.3.2.conv1_1x1.conv.weight\", \"encoder.stages.3.2.conv1_1x1.bn.weight\", \"encoder.stages.3.2.conv1_1x1.bn.bias\", \"encoder.stages.3.2.conv1_1x1.bn.running_mean\", \"encoder.stages.3.2.conv1_1x1.bn.running_var\", \"encoder.stages.3.2.conv1_1x1.bn.num_batches_tracked\", \"encoder.stages.3.2.conv2_kxk.conv.weight\", \"encoder.stages.3.2.conv2_kxk.bn.weight\", \"encoder.stages.3.2.conv2_kxk.bn.bias\", \"encoder.stages.3.2.conv2_kxk.bn.running_mean\", \"encoder.stages.3.2.conv2_kxk.bn.running_var\", \"encoder.stages.3.2.conv2_kxk.bn.num_batches_tracked\", \"encoder.stages.3.2.conv3_1x1.conv.weight\", \"encoder.stages.3.2.conv3_1x1.bn.weight\", \"encoder.stages.3.2.conv3_1x1.bn.bias\", \"encoder.stages.3.2.conv3_1x1.bn.running_mean\", \"encoder.stages.3.2.conv3_1x1.bn.running_var\", \"encoder.stages.3.2.conv3_1x1.bn.num_batches_tracked\", \"encoder.stages.3.3.conv1_1x1.conv.weight\", \"encoder.stages.3.3.conv1_1x1.bn.weight\", \"encoder.stages.3.3.conv1_1x1.bn.bias\", \"encoder.stages.3.3.conv1_1x1.bn.running_mean\", \"encoder.stages.3.3.conv1_1x1.bn.running_var\", \"encoder.stages.3.3.conv1_1x1.bn.num_batches_tracked\", \"encoder.stages.3.3.conv2_kxk.conv.weight\", \"encoder.stages.3.3.conv2_kxk.bn.weight\", \"encoder.stages.3.3.conv2_kxk.bn.bias\", \"encoder.stages.3.3.conv2_kxk.bn.running_mean\", \"encoder.stages.3.3.conv2_kxk.bn.running_var\", \"encoder.stages.3.3.conv2_kxk.bn.num_batches_tracked\", \"encoder.stages.3.3.conv3_1x1.conv.weight\", \"encoder.stages.3.3.conv3_1x1.bn.weight\", \"encoder.stages.3.3.conv3_1x1.bn.bias\", \"encoder.stages.3.3.conv3_1x1.bn.running_mean\", \"encoder.stages.3.3.conv3_1x1.bn.running_var\", \"encoder.stages.3.3.conv3_1x1.bn.num_batches_tracked\", \"encoder.stages.3.4.conv1_1x1.conv.weight\", \"encoder.stages.3.4.conv1_1x1.bn.weight\", \"encoder.stages.3.4.conv1_1x1.bn.bias\", \"encoder.stages.3.4.conv1_1x1.bn.running_mean\", \"encoder.stages.3.4.conv1_1x1.bn.running_var\", \"encoder.stages.3.4.conv1_1x1.bn.num_batches_tracked\", \"encoder.stages.3.4.conv2_kxk.conv.weight\", \"encoder.stages.3.4.conv2_kxk.bn.weight\", \"encoder.stages.3.4.conv2_kxk.bn.bias\", \"encoder.stages.3.4.conv2_kxk.bn.running_mean\", \"encoder.stages.3.4.conv2_kxk.bn.running_var\", \"encoder.stages.3.4.conv2_kxk.bn.num_batches_tracked\", \"encoder.stages.3.4.conv3_1x1.conv.weight\", \"encoder.stages.3.4.conv3_1x1.bn.weight\", \"encoder.stages.3.4.conv3_1x1.bn.bias\", \"encoder.stages.3.4.conv3_1x1.bn.running_mean\", \"encoder.stages.3.4.conv3_1x1.bn.running_var\", \"encoder.stages.3.4.conv3_1x1.bn.num_batches_tracked\", \"encoder.stages.4.0.conv1_1x1.conv.weight\", \"encoder.stages.4.0.conv1_1x1.bn.weight\", \"encoder.stages.4.0.conv1_1x1.bn.bias\", \"encoder.stages.4.0.conv1_1x1.bn.running_mean\", \"encoder.stages.4.0.conv1_1x1.bn.running_var\", \"encoder.stages.4.0.conv1_1x1.bn.num_batches_tracked\", \"encoder.stages.4.0.conv2_kxk.conv.weight\", \"encoder.stages.4.0.conv2_kxk.bn.weight\", \"encoder.stages.4.0.conv2_kxk.bn.bias\", \"encoder.stages.4.0.conv2_kxk.bn.running_mean\", \"encoder.stages.4.0.conv2_kxk.bn.running_var\", \"encoder.stages.4.0.conv2_kxk.bn.num_batches_tracked\", \"encoder.stages.4.0.conv3_1x1.conv.weight\", \"encoder.stages.4.0.conv3_1x1.bn.weight\", \"encoder.stages.4.0.conv3_1x1.bn.bias\", \"encoder.stages.4.0.conv3_1x1.bn.running_mean\", \"encoder.stages.4.0.conv3_1x1.bn.running_var\", \"encoder.stages.4.0.conv3_1x1.bn.num_batches_tracked\", \"encoder.stages.4.1.conv1_1x1.conv.weight\", \"encoder.stages.4.1.conv1_1x1.bn.weight\", \"encoder.stages.4.1.conv1_1x1.bn.bias\", \"encoder.stages.4.1.conv1_1x1.bn.running_mean\", \"encoder.stages.4.1.conv1_1x1.bn.running_var\", \"encoder.stages.4.1.conv1_1x1.bn.num_batches_tracked\", \"encoder.stages.4.1.conv2_kxk.conv.weight\", \"encoder.stages.4.1.conv2_kxk.bn.weight\", \"encoder.stages.4.1.conv2_kxk.bn.bias\", \"encoder.stages.4.1.conv2_kxk.bn.running_mean\", \"encoder.stages.4.1.conv2_kxk.bn.running_var\", \"encoder.stages.4.1.conv2_kxk.bn.num_batches_tracked\", \"encoder.stages.4.1.conv3_1x1.conv.weight\", \"encoder.stages.4.1.conv3_1x1.bn.weight\", \"encoder.stages.4.1.conv3_1x1.bn.bias\", \"encoder.stages.4.1.conv3_1x1.bn.running_mean\", \"encoder.stages.4.1.conv3_1x1.bn.running_var\", \"encoder.stages.4.1.conv3_1x1.bn.num_batches_tracked\", \"encoder.stages.4.2.conv1_1x1.conv.weight\", \"encoder.stages.4.2.conv1_1x1.bn.weight\", \"encoder.stages.4.2.conv1_1x1.bn.bias\", \"encoder.stages.4.2.conv1_1x1.bn.running_mean\", \"encoder.stages.4.2.conv1_1x1.bn.running_var\", \"encoder.stages.4.2.conv1_1x1.bn.num_batches_tracked\", \"encoder.stages.4.2.conv2_kxk.conv.weight\", \"encoder.stages.4.2.conv2_kxk.bn.weight\", \"encoder.stages.4.2.conv2_kxk.bn.bias\", \"encoder.stages.4.2.conv2_kxk.bn.running_mean\", \"encoder.stages.4.2.conv2_kxk.bn.running_var\", \"encoder.stages.4.2.conv2_kxk.bn.num_batches_tracked\", \"encoder.stages.4.2.conv3_1x1.conv.weight\", \"encoder.stages.4.2.conv3_1x1.bn.weight\", \"encoder.stages.4.2.conv3_1x1.bn.bias\", \"encoder.stages.4.2.conv3_1x1.bn.running_mean\", \"encoder.stages.4.2.conv3_1x1.bn.running_var\", \"encoder.stages.4.2.conv3_1x1.bn.num_batches_tracked\", \"encoder.stages.4.3.conv1_1x1.conv.weight\", \"encoder.stages.4.3.conv1_1x1.bn.weight\", \"encoder.stages.4.3.conv1_1x1.bn.bias\", \"encoder.stages.4.3.conv1_1x1.bn.running_mean\", \"encoder.stages.4.3.conv1_1x1.bn.running_var\", \"encoder.stages.4.3.conv1_1x1.bn.num_batches_tracked\", \"encoder.stages.4.3.conv2_kxk.conv.weight\", \"encoder.stages.4.3.conv2_kxk.bn.weight\", \"encoder.stages.4.3.conv2_kxk.bn.bias\", \"encoder.stages.4.3.conv2_kxk.bn.running_mean\", \"encoder.stages.4.3.conv2_kxk.bn.running_var\", \"encoder.stages.4.3.conv2_kxk.bn.num_batches_tracked\", \"encoder.stages.4.3.conv3_1x1.conv.weight\", \"encoder.stages.4.3.conv3_1x1.bn.weight\", \"encoder.stages.4.3.conv3_1x1.bn.bias\", \"encoder.stages.4.3.conv3_1x1.bn.running_mean\", \"encoder.stages.4.3.conv3_1x1.bn.running_var\", \"encoder.stages.4.3.conv3_1x1.bn.num_batches_tracked\", \"encoder.final_conv.conv.weight\", \"encoder.final_conv.bn.weight\", \"encoder.final_conv.bn.bias\", \"encoder.final_conv.bn.running_mean\", \"encoder.final_conv.bn.running_var\", \"encoder.final_conv.bn.num_batches_tracked\". \n\tsize mismatch for decoder.blocks.0.conv1.0.weight: copying a param with shape torch.Size([256, 3200, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 1072, 3, 3]).\n\tsize mismatch for decoder.blocks.1.conv1.0.weight: copying a param with shape torch.Size([128, 448, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 296, 3, 3]).\n\tsize mismatch for decoder.blocks.2.conv1.0.weight: copying a param with shape torch.Size([64, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 152, 3, 3]).\n\tsize mismatch for decoder.blocks.3.conv1.0.weight: copying a param with shape torch.Size([32, 96, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 80, 3, 3]).","output_type":"error"}]},{"cell_type":"markdown","source":"# 📈 Visualization","metadata":{}},{"cell_type":"code","source":"if debug:\n    for img, msk in zip(imgs[0][:5], msks[0][:5]):\n        plt.figure(figsize=(12, 7))\n        \n        plt.subplot(1, 3, 1); plt.imshow(img, cmap='bone');\n        plt.axis('OFF'); plt.title('image')\n        plt.subplot(1, 3, 2); plt.imshow(msk*255); plt.axis('OFF'); plt.title('mask')\n        plt.subplot(1, 3, 3); plt.imshow(img, cmap='bone'); plt.imshow(msk*255, alpha=0.4);\n        plt.axis('OFF'); plt.title('overlay')\n        plt.tight_layout()\n        plt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-07-03T02:58:25.832361Z","iopub.status.idle":"2022-07-03T02:58:25.832965Z","shell.execute_reply.started":"2022-07-03T02:58:25.832722Z","shell.execute_reply":"2022-07-03T02:58:25.832755Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del imgs, msks\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-07-03T02:58:25.834129Z","iopub.status.idle":"2022-07-03T02:58:25.834687Z","shell.execute_reply.started":"2022-07-03T02:58:25.834455Z","shell.execute_reply":"2022-07-03T02:58:25.834479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 📝 Submission","metadata":{}},{"cell_type":"code","source":"pred_df = pd.DataFrame({\n    \"id\":pred_ids,\n    \"class\":pred_classes,\n    \"predicted\":pred_strings\n})\nif not debug:\n    sub_df = pd.read_csv('../input/uw-madison-gi-tract-image-segmentation/sample_submission.csv')\n    del sub_df['predicted']\nelse:\n    sub_df = pd.read_csv('../input/uw-madison-gi-tract-image-segmentation/train.csv')[:1000*3]\n    del sub_df['segmentation']\n    \nsub_df = sub_df.merge(pred_df, on=['id','class'])\nsub_df.to_csv('submission.csv',index=False)\ndisplay(sub_df.head(5))","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-07-03T02:04:11.499503Z","iopub.status.idle":"2022-07-03T02:04:11.499887Z","shell.execute_reply.started":"2022-07-03T02:04:11.499680Z","shell.execute_reply":"2022-07-03T02:04:11.499700Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}